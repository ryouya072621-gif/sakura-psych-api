from flask import Flask, request, jsonify, send_from_directory, Response
import os
import smtplib
from email.message import EmailMessage
from datetime import datetime
import json
import uuid
import csv
import io

from pathlib import Path
import pandas as pd
import numpy as np

# ===== Firebase/Firestore =====
import firebase_admin
from firebase_admin import credentials, firestore

# FirebaseåˆæœŸåŒ–ï¼ˆCloud Runç’°å¢ƒã§ã¯è‡ªå‹•èªè¨¼ï¼‰
if not firebase_admin._apps:
    firebase_admin.initialize_app()

db = firestore.client()
FIRESTORE_COLLECTION = "diagnosis_results"

# ===== Flask ã‚¢ãƒ—ãƒªè¨­å®š =====
app = Flask(__name__, static_folder=".", static_url_path="")

# ===== ãƒ¡ãƒ¼ãƒ«è¨­å®šï¼ˆCloud Run ã®ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰ =====
SMTP_HOST = os.environ.get("SMTP_HOST")
SMTP_PORT = int(os.environ.get("SMTP_PORT", "587"))
SMTP_USER = os.environ.get("SMTP_USER")
SMTP_PASS = os.environ.get("SMTP_PASS")
MAIL_FROM = os.environ.get("MAIL_FROM")
MAIL_HR = os.environ.get("MAIL_HR")

# =========================
# ãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿ã®èª­è¾¼è¨­å®š
# =========================
BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "model_data"
RESULTS_DIR = BASE_DIR / "results"
RESULTS_DIR.mkdir(exist_ok=True)

# è³ªå•ID â†’ å› å­ãƒ©ãƒ™ãƒ«
question_factor_df = pd.read_csv(DATA_DIR / "question_to_factor_mapping.csv")

# =========================
# è»¸ã®æ—¥æœ¬èªåãƒãƒƒãƒ”ãƒ³ã‚°
# =========================
AXIS_NAMES_JP = {
    "PC1": "å’Œå‹",      # å”èª¿æ€§ãƒ»ã‚µãƒãƒ¼ãƒˆå¿—å‘
    "PC2": "é™½å‹",      # ç¤¾äº¤æ€§ãƒ»ç™ºä¿¡åŠ›
    "PC3": "ç†å‹",      # è«–ç†æ€§ãƒ»æ…é‡ã•
    "PC4": "å°å‹",      # ä¸»ä½“æ€§ãƒ»ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—
}

AXIS_DESCRIPTIONS = {
    "PC1": "å‘¨å›²ã¨ã®èª¿å’Œã‚’å¤§åˆ‡ã«ã—ã€ã‚µãƒãƒ¼ãƒˆå½¹ã¨ã—ã¦åŠ›ã‚’ç™ºæ®ã™ã‚‹ã‚¿ã‚¤ãƒ—",
    "PC2": "äººã¨ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¥½ã—ã¿ã€å ´ã‚’ç››ã‚Šä¸Šã’ã‚‹ã‚¿ã‚¤ãƒ—",
    "PC3": "è«–ç†çš„ã«è€ƒãˆã€æ…é‡ã«ç‰©äº‹ã‚’é€²ã‚ã‚‹ã‚¿ã‚¤ãƒ—",
    "PC4": "è‡ªã‚‰ç‡å…ˆã—ã¦å‹•ãã€ãƒãƒ¼ãƒ ã‚’å¼•ã£å¼µã‚‹ã‚¿ã‚¤ãƒ—",
}

# =========================
# ä¼šç¤¾ãƒ»æ‰€å±ã‚«ãƒ†ã‚´ãƒª
# =========================
COMPANY_CATEGORIES = [
    "ã•ãã‚‰ä¼šåœ¨ç±ã‚¹ã‚¿ãƒƒãƒ•",
    "ä¸­é€”é¸è€ƒ",
    "æ­¯å­¦éƒ¨ç”Ÿãƒ»ç ”ä¿®åŒ»",
    "è¡›ç”Ÿå£«å­¦ç”Ÿ",
    "å¤§å­¦ç”Ÿ",
    "é«˜æ ¡ç”Ÿ",
    "ãã®ä»–"
]

# =========================
# ã‚¿ã‚°ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# =========================
TAG_MASTER_PATH = DATA_DIR / "tag_master.xlsx"

def load_tag_master():
    """ã‚¿ã‚°ãƒã‚¹ã‚¿ãƒ¼ã‚’Excelã‹ã‚‰èª­ã¿è¾¼ã¿"""
    if TAG_MASTER_PATH.exists():
        try:
            df = pd.read_excel(TAG_MASTER_PATH)
            result = {}
            for category in df['category'].unique():
                result[category] = df[df['category'] == category]['value'].tolist()
            return result
        except Exception as e:
            app.logger.warning(f"tag_master.xlsxèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    return {
        "clinics": [
            "ã•ãã‚‰æ­¯ç§‘", "ãŸã‚“ã½ã½æ­¯ç§‘", "ã‚ã‚Šã™æ­¯ç§‘", "æ˜¥æ—¥äº•ãã‚‰ã‚Šæ­¯ç§‘", "æ¾æˆ¸ã‚ã‚Šã™æ­¯ç§‘",
            "æ± ä¸‹ã•ãã‚‰æ­¯ç§‘", "æ—¥é€²èµ¤æ± ãŸã‚“ã½ã½æ­¯ç§‘", "æ˜¥æ—¥äº•ã‚¢ãƒƒãƒ—ãƒ«æ­¯ç§‘", "ç·‘åŒºã•ãã‚‰åŒ»é™¢ãƒ»æ­¯ç§‘",
            "ç·‘åŒºã•ãã‚‰åŒ»é™¢ãƒ»åŒ»ç§‘", "ç·‘åŒºã•ãã‚‰åŒ»é™¢ãƒ»å¥è¨º", "é‡‘æ²¢ã•ãã‚‰åŒ»é™¢ãƒ»å¥è¨º", "é‡‘æ²¢ã•ãã‚‰åŒ»é™¢ãƒ»å©¦äººç§‘",
            "ãƒãƒ”ãƒã‚¹æ­¯ç§‘", "æµå±±ãƒãƒ”ãƒã‚¹æ­¯ç§‘", "ã‚¤ãƒ¼ã‚¢ã‚¹æ˜¥æ—¥äº•", "åé§…ã•ãã‚‰åŒ»é™¢ãƒ»åå¤å±‹æ­¯ç§‘ãƒ»æ­¯ç§‘",
            "åé§…ã•ãã‚‰åŒ»é™¢ãƒ»åå¤å±‹æ­¯ç§‘ãƒ»å†…ç§‘", "åé§…ã•ãã‚‰åŒ»é™¢ãƒ»åå¤å±‹æ­¯ç§‘ãƒ»çš®è†šç§‘", "ãã‚‰ã‚Šå¤§æ£®æ­¯ç§‘",
            "ã‚¯ãƒ­ãƒ¼ãƒãƒ¼æ­¯ç§‘", "æµå±±ã‚ã‚Šã™æ­¯ç§‘ãƒ»çŸ¯æ­£æ­¯ç§‘", "é•·ä¹…æ‰‹ã•ãã‚‰æ­¯ç§‘ãƒ»çŸ¯æ­£æ­¯ç§‘", "ãƒ’ãƒ­ãƒ‡ãƒ³ã‚¿ãƒ«",
            "ç”ºå±‹ã•ãã‚‰æ­¯ç§‘ãƒ»çŸ¯æ­£æ­¯ç§‘", "é‡‘æ²¢ã•ãã‚‰åŒ»é™¢ æ­¯ç§‘", "ããŸçŸ¯æ­£æ­¯ç§‘ã‚¯ãƒªãƒ‹ãƒƒã‚¯", "ãƒ‡ãƒ³ã‚¿ãƒ«ã‚ªãƒ•ã‚£ã‚¹å¢—ç”°",
            "ç¥¥å—æ­¯ç§‘ãƒ»çŸ¯æ­£æ­¯ç§‘åŒ»é™¢", "ä¸‰ç”°çŸ¯æ­£æ­¯ç§‘åŒ»é™¢", "ã‚°ãƒ©ãƒ³ãƒ‰æ­¯ç§‘åŒ»é™¢", "è¨ªå•éƒ¨ï¼ˆæ˜¥æ—¥äº•äº‹å‹™æ‰€ï¼‰",
            "è¨ªå•éƒ¨ï¼ˆæä¸­äº‹å‹™æ‰€ï¼‰", "æ˜¥æ—¥äº•äº‹å‹™å±€", "åå¤å±‹é§…JPã‚¿ãƒ¯ãƒ¼äº‹å‹™æ‰€", "ã„ã‚Šãªã‹äº‹å‹™æ‰€",
            "æ± ä¸‹äº‹å‹™æ‰€", "æµå±±äº‹å‹™æ‰€", "æµ„å¿ƒäº‹å‹™æ‰€"
        ],
        "positions": ["æ­¯ç§‘åŒ»å¸«", "æ­¯ç§‘è¡›ç”Ÿå£«", "æ­¯ç§‘åŠ©æ‰‹", "å—ä»˜", "äº‹å‹™", "ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"],
        "areas": [],
        "status": ["active", "leave", "retired"]
    }

def save_tag_master(tag_master: dict):
    """ã‚¿ã‚°ãƒã‚¹ã‚¿ãƒ¼ã‚’Excelã«ä¿å­˜"""
    records = []
    for category, values in tag_master.items():
        for value in values:
            records.append({"category": category, "value": value})
    df = pd.DataFrame(records)
    df.to_excel(TAG_MASTER_PATH, index=False)

TAG_MASTER = load_tag_master()

# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ©ãƒ™ãƒ«
STATUS_LABELS = {
    "active": "åœ¨è·",
    "leave": "ä¼‘è·",
    "retired": "é€€è·æ¸ˆã¿"
}

# -----------------------------
# A01ã€œB36 â†’ 0ã€œ71 ã¸ã® index å¤‰æ›
# -----------------------------
def question_id_to_index(qid: str) -> int:
    qid = str(qid).strip().upper()
    if len(qid) < 3:
        raise ValueError(f"question_id ã®å½¢å¼ãŒä¸æ­£ã§ã™: {qid}")
    prefix = qid[0]
    try:
        num = int(qid[1:])
    except ValueError:
        raise ValueError(f"question_id ã®ç•ªå·éƒ¨åˆ†ãŒæ•°å€¤ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {qid}")
    if not (1 <= num <= 36):
        raise ValueError(f"question_id ã®ç•ªå·ãŒ 1ã€œ36 ã®ç¯„å›²å¤–ã§ã™: {qid}")
    if prefix == "A":
        return num - 1
    elif prefix == "B":
        return 36 + (num - 1)
    else:
        raise ValueError(f"question_id ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒ A/B ä»¥å¤–ã§ã™: {qid}")


# -----------------------------
# 72å• â†’ 55å› å­ã¸ã®å¤‰æ›
# -----------------------------
# è³ªå•æ–‡æ”¹å–„ã§å·¦å³é †åºãŒé€†è»¢ã—ãŸè³ªå•ï¼ˆå›ç­”å€¤ã‚’åè»¢: 1â†”5, 2â†”4, 3â†’3ï¼‰
# A25 (index 24), B14 (index 49), B32 (index 67)
REVERSE_QUESTION_INDICES = [24, 49, 67]

def build_55factor_features_from_answers(answers):
    if not isinstance(answers, (list, tuple)):
        raise ValueError("answers ã¯ list ã‹ tuple ã§æ¸¡ã—ã¦ãã ã•ã„ã€‚")
    if len(answers) != 72:
        raise ValueError(f"answers ã®é•·ã•ãŒ 72 ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆlen={len(answers)}ï¼‰ã€‚")
    try:
        vals = [float(x) for x in answers]
    except Exception:
        raise ValueError("answers å†…ã«æ•°å€¤ã«å¤‰æ›ã§ããªã„å€¤ãŒã‚ã‚Šã¾ã™ã€‚")

    # å·¦å³é †åºãŒé€†è»¢ã—ãŸè³ªå•ã®å›ç­”å€¤ã‚’åè»¢
    for idx in REVERSE_QUESTION_INDICES:
        if 0 <= idx < len(vals):
            vals[idx] = 6 - vals[idx]  # 1â†’5, 2â†’4, 3â†’3, 4â†’2, 5â†’1

    df = question_factor_df.copy()
    df.columns = [str(c).strip().replace("\ufeff", "") for c in df.columns]
    if "question_id" not in df.columns or "factor" not in df.columns:
        raise ValueError("question_to_factor_mapping.csv ã« 'question_id' ã¾ãŸã¯ 'factor' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    records = []
    for _, row in df.iterrows():
        qid = row["question_id"]
        factor = str(row["factor"]).strip()
        try:
            idx = question_id_to_index(qid)
        except Exception:
            continue
        if not (0 <= idx < len(vals)):
            continue
        try:
            v = float(vals[idx])
        except Exception:
            continue
        records.append({"factor": factor, "value": v})

    if not records:
        raise ValueError("answers ã¨ question_to_factor_mapping.csv ã®å¯¾å¿œã‹ã‚‰ 55å› å­ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    rec_df = pd.DataFrame(records)
    factor_series = rec_df.groupby("factor")["value"].mean()
    return factor_series.to_dict()


# æ—§ï¼š4è»¸ãƒ–ãƒ­ãƒƒã‚¯å¹³å‡
def build_features_from_answers(answers):
    if not isinstance(answers, (list, tuple)):
        raise ValueError("answers ã¯ list ã‹ tuple ã§æ¸¡ã—ã¦ãã ã•ã„ã€‚")
    if len(answers) != 72:
        raise ValueError(f"answers ã®é•·ã•ãŒ 72 ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆlen={len(answers)}ï¼‰ã€‚")
    try:
        vals = [float(x) for x in answers]
    except Exception:
        raise ValueError("answers å†…ã«æ•°å€¤ã«å¤‰æ›ã§ããªã„å€¤ãŒã‚ã‚Šã¾ã™ã€‚")

    def avg(s, e):
        return sum(vals[s:e]) / (e - s)

    return {
        "ã‚¹ãƒˆãƒ¬ã‚¹ã«å¯¾ã™ã‚‹å¼±ã•": avg(0, 18),
        "å¤–å‘å‹/è‡ªå•å‹": avg(18, 36),
        "è«–ç†é‡è¦–/æƒ³ã„é‡è¦–": avg(36, 54),
        "å”èª¿å‹/ç«¶äº‰å‹": avg(54, 72),
    }


# ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
try:
    mean_std_df = pd.read_excel(DATA_DIR / "mean_std_55.xlsx")
    app.logger.info("mean_std_55.xlsx ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ55å› å­ç‰ˆï¼‰")
except Exception:
    mean_std_df = pd.read_excel(DATA_DIR / "mean_std_pca_features.xlsx")
    app.logger.warning("mean_std_55.xlsx ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€mean_std_pca_features.xlsxï¼ˆæ—§4è»¸ç‰ˆï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

try:
    pca_loadings_df = pd.read_excel(DATA_DIR / "pca_loadings_55.xlsx")
    app.logger.info("pca_loadings_55.xlsx ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ55å› å­ç‰ˆï¼‰")
except Exception:
    try:
        pca_loadings_df = pd.read_excel(DATA_DIR / "pca_loadings_clean.xlsx")
    except FileNotFoundError:
        pca_loadings_df = pd.read_excel(DATA_DIR / "pca_loadings.xlsx")

cluster_centers_df = pd.read_excel(DATA_DIR / "cluster_centers.xlsx")

# é©è·å‚ç…§ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãªã‘ã‚Œã°åˆæœŸãƒ‡ãƒ¼ã‚¿ä½œæˆï¼‰
JOB_FIT_FILE = DATA_DIR / "job_fit_profiles.xlsx"

def init_job_fit_data():
    """é©è·å‚ç…§ãƒ‡ãƒ¼ã‚¿ãŒãªã‘ã‚Œã°åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    if JOB_FIT_FILE.exists():
        return pd.read_excel(JOB_FIT_FILE)

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è·ç¨®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPC1ã€œPC4ã®ç†æƒ³å€¤ï¼‰
    default_jobs = [
        {"job_name": "å—ä»˜ãƒ»äº‹å‹™", "PC1": 15.0, "PC2": 5.0, "PC3": 10.0, "PC4": -5.0, "description": "ä¸å¯§ã§å”èª¿æ€§ãŒé«˜ãã€å®‰å®šã—ãŸå¯¾å¿œãŒã§ãã‚‹"},
        {"job_name": "æ­¯ç§‘è¡›ç”Ÿå£«", "PC1": 10.0, "PC2": 8.0, "PC3": 12.0, "PC4": 5.0, "description": "æ‚£è€…å¯¾å¿œã¨å°‚é–€ã‚¹ã‚­ãƒ«ã®ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ã‚‹"},
        {"job_name": "æ­¯ç§‘åŒ»å¸«", "PC1": 5.0, "PC2": 10.0, "PC3": 15.0, "PC4": 15.0, "description": "å°‚é–€æ€§ã¨ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã‚’å…¼ã­å‚™ãˆã¦ã„ã‚‹"},
        {"job_name": "æ­¯ç§‘åŠ©æ‰‹", "PC1": 18.0, "PC2": 3.0, "PC3": 8.0, "PC4": -3.0, "description": "ã‚µãƒãƒ¼ãƒˆåŠ›ãŒé«˜ãã€ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’å¤§åˆ‡ã«ã™ã‚‹"},
        {"job_name": "ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼", "PC1": 8.0, "PC2": 12.0, "PC3": 8.0, "PC4": 18.0, "description": "äººã‚’å‹•ã‹ã—ã€çµ„ç¹”ã‚’å¼•ã£å¼µã‚‹ãƒªãƒ¼ãƒ€ãƒ¼ã‚¿ã‚¤ãƒ—"},
        {"job_name": "ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼", "PC1": 20.0, "PC2": 10.0, "PC3": 5.0, "PC4": 0.0, "description": "å‚¾è´åŠ›ãŒé«˜ãã€äººã®æ°—æŒã¡ã«å¯„ã‚Šæ·»ãˆã‚‹"},
        {"job_name": "æŠ€å·¥å£«", "PC1": 3.0, "PC2": -5.0, "PC3": 18.0, "PC4": 5.0, "description": "ç·»å¯†ãªä½œæ¥­ã¨è«–ç†çš„æ€è€ƒãŒå¾—æ„"},
        {"job_name": "åºƒå ±ãƒ»ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "PC1": 5.0, "PC2": 18.0, "PC3": 10.0, "PC4": 10.0, "description": "ç™ºä¿¡åŠ›ã¨ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã«å„ªã‚Œã‚‹"},
    ]
    df = pd.DataFrame(default_jobs)
    df.to_excel(JOB_FIT_FILE, index=False)
    app.logger.info("é©è·å‚ç…§ãƒ‡ãƒ¼ã‚¿ã‚’åˆæœŸä½œæˆã—ã¾ã—ãŸ: job_fit_profiles.xlsx")
    return df

job_fit_df = init_job_fit_data()

# åˆ—åæ­£è¦åŒ–
mean_std_df = mean_std_df.rename(columns={
    mean_std_df.columns[0]: "feature",
    mean_std_df.columns[1]: "mean",
    mean_std_df.columns[2]: "std",
})

pca_loadings_df = pca_loadings_df.rename(columns={
    pca_loadings_df.columns[0]: "feature",
    pca_loadings_df.columns[1]: "PC1",
    pca_loadings_df.columns[2]: "PC2",
    pca_loadings_df.columns[3]: "PC3",
    pca_loadings_df.columns[4]: "PC4",
})

cluster_centers_df = cluster_centers_df.rename(columns={
    cluster_centers_df.columns[0]: "cluster_id",
    cluster_centers_df.columns[1]: "PC1",
    cluster_centers_df.columns[2]: "PC2",
    cluster_centers_df.columns[3]: "PC3",
    cluster_centers_df.columns[4]: "PC4",
})

# PCã‚¹ã‚±ãƒ¼ãƒ«è£œæ­£
PC_SCALE = {
    "PC1": 22.0,
    "PC2": 18.0,
    "PC3": 20.0,
    "PC4": 4.0,
}


# =========================
# ãƒ¢ãƒ‡ãƒ«è¨ˆç®—ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =========================
def compute_pc_and_type_from_features(raw_features: dict) -> dict:
    df = mean_std_df[["feature", "mean", "std"]].copy()
    df["value"] = df["feature"].map(raw_features)

    missing = df[df["value"].isna()]["feature"].tolist()
    if missing:
        for feat in missing:
            df.loc[df["feature"] == feat, "value"] = df.loc[df["feature"] == feat, "mean"]

    df["z"] = (df["value"] - df["mean"]) / df["std"]

    merged = pd.merge(
        df[["feature", "z"]],
        pca_loadings_df[["feature", "PC1", "PC2", "PC3", "PC4"]],
        on="feature",
        how="inner",
    )

    if merged.empty:
        raise ValueError("PCA ã®ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨çµåˆã—ãŸçµæœãŒç©ºã§ã™ã€‚feature åã®å¯¾å¿œã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    pcs_raw = {}
    pcs = {}
    for pc in ["PC1", "PC2", "PC3", "PC4"]:
        raw_pc = float((merged["z"] * merged[pc]).sum())
        pcs_raw[pc] = raw_pc
        scale = PC_SCALE.get(pc, 1.0)
        pcs[pc] = raw_pc * scale

    centers = cluster_centers_df[["cluster_id", "PC1", "PC2", "PC3", "PC4"]].copy()
    diffs = centers[["PC1", "PC2", "PC3", "PC4"]] - np.array(
        [pcs["PC1"], pcs["PC2"], pcs["PC3"], pcs["PC4"]]
    )
    dists = np.sqrt((diffs ** 2).sum(axis=1))

    best_idx = int(dists.idxmin())
    best_row = centers.loc[best_idx]
    cluster_id = int(best_row["cluster_id"])

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼IDã‹ã‚‰ç›´æ¥ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®šï¼ˆ4ã‚¿ã‚¤ãƒ—å‡ç­‰åˆ†å¸ƒï¼‰
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼0â†’Då‹, 1â†’Så‹, 2â†’Cå‹, 3â†’På‹
    cluster_to_type = {0: "D", 1: "S", 2: "C", 3: "P"}
    main_type = cluster_to_type.get(cluster_id, "S")

    return {
        "PC1": pcs["PC1"],
        "PC2": pcs["PC2"],
        "PC3": pcs["PC3"],
        "PC4": pcs["PC4"],
        "cluster_id": cluster_id,
        "type": main_type,
    }


# =========================
# ãƒ¬ãƒ™ãƒ«åˆ¤å®šï¼ˆ5æ®µéšï¼‰
# =========================
def get_level_label(value: float) -> str:
    """PCã‚¹ã‚³ã‚¢ã‚’5æ®µéšã®ãƒ©ãƒ™ãƒ«ã«å¤‰æ›"""
    if value >= 1.0:
        return "é«˜ã„"
    elif value >= 0.4:
        return "ã‚„ã‚„é«˜ã„"
    elif value <= -1.0:
        return "ä½ã„"
    elif value <= -0.4:
        return "ã‚„ã‚„ä½ã„"
    else:
        return "å¹³å‡"


def get_level_index(value: float) -> int:
    """PCã‚¹ã‚³ã‚¢ã‚’0-4ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›ï¼ˆä½ã„=0, é«˜ã„=4ï¼‰"""
    if value >= 1.0:
        return 4
    elif value >= 0.4:
        return 3
    elif value <= -1.0:
        return 0
    elif value <= -0.4:
        return 1
    else:
        return 2


# =========================
# å›ç­”ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯
# =========================
def calculate_consistency_score(answers: list) -> dict:
    """
    å›ç­”ã®ä¸€è²«æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    é¡ä¼¼è³ªå•é–“ã§ã®å›ç­”ã®ã°ã‚‰ã¤ãã‚’æ¤œå‡º
    """
    if not answers or len(answers) != 72:
        return {"score": 0, "details": [], "warning": "å›ç­”ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ã§ã™"}

    # é–¢é€£è³ªå•ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆé¡ä¼¼ã®å†…å®¹ã‚’å•ã†è³ªå•ï¼‰
    related_groups = [
        # ã‚¹ãƒˆãƒ¬ã‚¹é–¢é€£
        {"questions": [0, 6, 7, 8, 9, 10, 11, 12, 14, 16, 17], "name": "ã‚¹ãƒˆãƒ¬ã‚¹å¯¾å‡¦"},
        # å¤–å‘æ€§é–¢é€£
        {"questions": [18, 19, 20, 24, 26, 27], "name": "å¤–å‘æ€§"},
        # ä¸»ä½“æ€§é–¢é€£
        {"questions": [32, 33, 35, 36, 37, 38, 39, 40, 41], "name": "ä¸»ä½“æ€§"},
        # ç¶™ç¶šæ€§é–¢é€£
        {"questions": [63, 64, 65, 66, 67, 68, 69, 70], "name": "ç¶™ç¶šæ€§"},
    ]

    inconsistencies = []
    total_variance = 0
    group_count = 0

    for group in related_groups:
        indices = group["questions"]
        values = [answers[i] for i in indices if i < len(answers)]
        if len(values) >= 3:
            variance = np.var(values)
            total_variance += variance
            group_count += 1

            # åˆ†æ•£ãŒ1.5ä»¥ä¸Šãªã‚‰ä¸ä¸€è‡´ã®å¯èƒ½æ€§
            if variance > 1.5:
                inconsistencies.append({
                    "group": group["name"],
                    "variance": round(variance, 2),
                    "message": f"{group['name']}ã«é–¢ã™ã‚‹å›ç­”ã«ã°ã‚‰ã¤ããŒã‚ã‚Šã¾ã™"
                })

    # æ¥µç«¯ãªå›ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå…¨ã¦åŒã˜å€¤ãªã©ï¼‰
    unique_answers = len(set(answers))
    if unique_answers <= 3:
        inconsistencies.append({
            "group": "å…¨ä½“",
            "message": "å›ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ¥µç«¯ã§ã™ï¼ˆã»ã¼åŒã˜å€¤ã§å›ç­”ã•ã‚Œã¦ã„ã¾ã™ï¼‰"
        })

    # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ã€é«˜ã„ã»ã©ä¸€è²«æ€§ã‚ã‚Šï¼‰
    avg_variance = total_variance / group_count if group_count > 0 else 0
    base_score = max(0, 100 - (avg_variance * 30))

    # æ¥µç«¯ãªå›ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
    if unique_answers <= 3:
        base_score = min(base_score, 50)

    score = int(round(base_score))

    return {
        "score": score,
        "details": inconsistencies,
        "warning": "å›ç­”ã«ä¸€è²«æ€§ãŒãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™" if score < 70 else None
    }


# =========================
# ã‚¹ãƒˆãƒ¬ã‚¹è€æ€§10æ®µéšè¨ˆç®—
# =========================
def calculate_stress_tolerance(features_55: dict, pc: dict) -> int:
    """
    55å› å­ã¨PCã‚¹ã‚³ã‚¢ã‹ã‚‰ã‚¹ãƒˆãƒ¬ã‚¹è€æ€§ã‚’10æ®µéšã§è¨ˆç®—
    """
    # ã‚¹ãƒˆãƒ¬ã‚¹é–¢é€£å› å­
    stress_factors = [
        "stress_tolerance",
        "stress_recovery", 
        "emotional_stability",
        "emotional_control",
        "calmness",
        "anxiety_tendency",  # é€†è»¢é …ç›®
        "stress_sensitivity",  # é€†è»¢é …ç›®
    ]
    
    score = 0
    count = 0
    
    for factor in stress_factors:
        if factor in features_55:
            val = features_55[factor]
            # é€†è»¢é …ç›®ã¯5ã‹ã‚‰å¼•ã
            if factor in ["anxiety_tendency", "stress_sensitivity"]:
                val = 6 - val
            score += val
            count += 1
    
    if count > 0:
        avg = score / count
        # 1-5ã‚¹ã‚±ãƒ¼ãƒ«ã‚’1-10ã‚¹ã‚±ãƒ¼ãƒ«ã«å¤‰æ›
        stress_10 = int(round((avg - 1) * 2.25 + 1))
        stress_10 = max(1, min(10, stress_10))
        return stress_10
    
    return 5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ


# =========================
# éš ã‚Œå±æ€§ãƒãƒƒã‚¸ç”Ÿæˆ
# =========================
def generate_badges(features_55: dict, pc: dict) -> list:
    """
    55å› å­ã¨PCå€¤ã‹ã‚‰éš ã‚Œå±æ€§ãƒãƒƒã‚¸ã‚’åˆ¤å®šã—ã¦è¿”ã™
    æœ€å¤§5å€‹ã¾ã§ã€ãƒ¬ã‚¢åº¦é †ã«é¸å‡º
    """
    all_badges = []

    # ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    def get(key, default=0):
        return features_55.get(key, default)

    # ========== ã‚«ãƒ†ã‚´ãƒª1: è¡Œå‹•ã‚¹ã‚¿ã‚¤ãƒ«ç³» ==========

    # ğŸš€ ã‚¹ã‚¿ãƒ¼ãƒˆãƒ€ãƒƒã‚·ãƒ£ãƒ¼ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("action_speed") >= 3.5 and get("decision_speed") >= 3.5:
        all_badges.append({
            "id": "start_dasher",
            "name": "ã‚¹ã‚¿ãƒ¼ãƒˆãƒ€ãƒƒã‚·ãƒ£ãƒ¼",
            "emoji": "ğŸš€",
            "category": "è¡Œå‹•ã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 2,
            "description": "å³æ–­å³æ±ºã§å‹•ãå‡ºã™ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ¢ ã˜ã£ãã‚Šæ´¾ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("action_cautiousness") >= 3.5 and get("cautiousness") >= 3.5:
        all_badges.append({
            "id": "careful_type",
            "name": "ã˜ã£ãã‚Šæ´¾",
            "emoji": "ğŸ¢",
            "category": "è¡Œå‹•ã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 2,
            "description": "æ…é‡ã«æº–å‚™ã—ã¦ã‹ã‚‰å‹•ãã‚¿ã‚¤ãƒ—"
        })

    # ğŸ¯ å®Œé‚ã®é¬¼ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("persistence") >= 3.5 and get("consistency") >= 3.5:
        all_badges.append({
            "id": "finisher",
            "name": "å®Œé‚ã®é¬¼",
            "emoji": "ğŸ¯",
            "category": "è¡Œå‹•ã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 2,
            "description": "ä¸€åº¦å§‹ã‚ãŸã‚‰ã‚„ã‚ŠæŠœãã‚¿ã‚¤ãƒ—"
        })

    # ğŸ¦‹ ãƒãƒ«ãƒã‚¿ã‚¹ã‚«ãƒ¼ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("flexibility") >= 3.5 and get("activity_level") >= 3.5:
        all_badges.append({
            "id": "multitasker",
            "name": "ãƒãƒ«ãƒã‚¿ã‚¹ã‚«ãƒ¼",
            "emoji": "ğŸ¦‹",
            "category": "è¡Œå‹•ã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 2,
            "description": "è¤‡æ•°ã®ã“ã¨ã‚’åŒæ™‚ã«ã“ãªã›ã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ“ æ®µå–ã‚Šãƒã‚¹ã‚¿ãƒ¼ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("planning") >= 3.5 and get("conscientiousness") >= 3.5:
        all_badges.append({
            "id": "planner",
            "name": "æ®µå–ã‚Šãƒã‚¹ã‚¿ãƒ¼",
            "emoji": "ğŸ“",
            "category": "è¡Œå‹•ã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 2,
            "description": "è¨ˆç”»ã‚’ç«‹ã¦ã¦ç€å®Ÿã«é€²ã‚ã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # âš¡ ç›´æ„Ÿã‚¢ã‚¯ã‚¿ãƒ¼ï¼ˆâ˜…â˜†â˜†ï¼‰
    if get("action_orientation") >= 3.5 and get("decision_speed") >= 3.5 and get("planning") < 3.5:
        all_badges.append({
            "id": "intuitive_actor",
            "name": "ç›´æ„Ÿã‚¢ã‚¯ã‚¿ãƒ¼",
            "emoji": "âš¡",
            "category": "è¡Œå‹•ã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 1,
            "description": "è€ƒãˆã‚‹ã‚ˆã‚Šå…ˆã«å‹•ãã‚¿ã‚¤ãƒ—"
        })

    # ========== ã‚«ãƒ†ã‚´ãƒª2: å¯¾äººã‚¹ã‚¿ã‚¤ãƒ«ç³» ==========

    # ğŸ¤ å…±æ„Ÿãƒã‚¤ã‚¹ã‚¿ãƒ¼ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("empathy") >= 3.5 and get("emotional_sensitivity") >= 3.5:
        all_badges.append({
            "id": "empathy_master",
            "name": "å…±æ„Ÿãƒã‚¤ã‚¹ã‚¿ãƒ¼",
            "emoji": "ğŸ¤",
            "category": "å¯¾äººã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 2,
            "description": "äººã®æ°—æŒã¡ã«å¯„ã‚Šæ·»ãˆã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ¤ å ´ã®ç››ã‚Šä¸Šã’å½¹ï¼ˆâ˜…â˜…â˜…ï¼‰
    if get("sociability") >= 3.5 and get("extroversion") >= 3.5 and get("social_ease") >= 3.5:
        all_badges.append({
            "id": "mood_maker",
            "name": "å ´ã®ç››ã‚Šä¸Šã’å½¹",
            "emoji": "ğŸ¤",
            "category": "å¯¾äººã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 3,
            "description": "ãƒ ãƒ¼ãƒ‰ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ§˜ ä¸€äººæ™‚é–“ã®é”äººï¼ˆâ˜…â˜†â˜†ï¼‰
    if get("inner_world") >= 3.5 and get("extroversion") < 3.5:
        all_badges.append({
            "id": "solo_master",
            "name": "ä¸€äººæ™‚é–“ã®é”äºº",
            "emoji": "ğŸ§˜",
            "category": "å¯¾äººã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 1,
            "description": "å†…çœãƒ»è‡ªåˆ†æ™‚é–“ã‚’å¤§åˆ‡ã«ã™ã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ‘‘ ãƒªãƒ¼ãƒ€ãƒ¼æ°—è³ªï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("leadership") >= 3.5 and get("self_efficacy") >= 3.5:
        all_badges.append({
            "id": "leader_type",
            "name": "ãƒªãƒ¼ãƒ€ãƒ¼æ°—è³ª",
            "emoji": "ğŸ‘‘",
            "category": "å¯¾äººã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 2,
            "description": "è‡ªç„¶ã¨äººã‚’å¼•ã£å¼µã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ¤² ã‚µãƒãƒ¼ã‚¿ãƒ¼é­‚ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("cooperativeness") >= 3.5 and get("obedience") >= 3.0 and get("leadership") < 3.5:
        all_badges.append({
            "id": "supporter",
            "name": "ã‚µãƒãƒ¼ã‚¿ãƒ¼é­‚",
            "emoji": "ğŸ¤²",
            "category": "å¯¾äººã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 2,
            "description": "ç¸ã®ä¸‹ã®åŠ›æŒã¡ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ—£ï¸ ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆãƒˆãƒ¼ã‚«ãƒ¼ï¼ˆâ˜…â˜†â˜†ï¼‰
    if get("straightforwardness") >= 3.5:
        all_badges.append({
            "id": "straight_talker",
            "name": "ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆãƒˆãƒ¼ã‚«ãƒ¼",
            "emoji": "ğŸ—£ï¸",
            "category": "å¯¾äººã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 1,
            "description": "æ€ã£ãŸã“ã¨ã‚’ç´ ç›´ã«è¨€ãˆã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ========== ã‚«ãƒ†ã‚´ãƒª3: æ€è€ƒã‚¹ã‚¿ã‚¤ãƒ«ç³» ==========

    # ğŸ”¬ ãƒ­ã‚¸ã‚«ãƒ«ã‚·ãƒ³ã‚«ãƒ¼ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("logical_thinking") >= 3.5 and get("cognitive_style") >= 3.5:
        all_badges.append({
            "id": "logical_thinker",
            "name": "ãƒ­ã‚¸ã‚«ãƒ«ã‚·ãƒ³ã‚«ãƒ¼",
            "emoji": "ğŸ”¬",
            "category": "æ€è€ƒã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 2,
            "description": "è«–ç†çš„ã«åˆ†æã§ãã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ’¡ ã‚¢ã‚¤ãƒ‡ã‚¢ãƒãƒ³ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("innovation_orientation") >= 3.5 and get("uniqueness") >= 3.5:
        all_badges.append({
            "id": "idea_person",
            "name": "ã‚¢ã‚¤ãƒ‡ã‚¢ãƒãƒ³",
            "emoji": "ğŸ’¡",
            "category": "æ€è€ƒã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 2,
            "description": "æ–°ã—ã„ç™ºæƒ³ãŒå¾—æ„ãªã‚¿ã‚¤ãƒ—"
        })

    # âš–ï¸ æ­£ç¾©ã®å‘³æ–¹ï¼ˆâ˜…â˜†â˜†ï¼‰
    if get("justice_sense") >= 4.0:
        all_badges.append({
            "id": "justice_seeker",
            "name": "æ­£ç¾©ã®å‘³æ–¹",
            "emoji": "âš–ï¸",
            "category": "æ€è€ƒã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 1,
            "description": "å…¬å¹³ã•ã‚’é‡è¦–ã™ã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ¨ æ„Ÿæ€§æ´¾ï¼ˆâ˜…â˜†â˜†ï¼‰
    if get("emotional_sensitivity") >= 3.5 and get("sensitivity") >= 3.5 and get("logical_thinking") < 3.5:
        all_badges.append({
            "id": "sensory_type",
            "name": "æ„Ÿæ€§æ´¾",
            "emoji": "ğŸ¨",
            "category": "æ€è€ƒã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 1,
            "description": "æ„Ÿè¦šã‚„ç›´æ„Ÿã‚’å¤§åˆ‡ã«ã™ã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ”„ æŸ”è»Ÿã‚·ãƒ•ã‚¿ãƒ¼ï¼ˆâ˜…â˜†â˜†ï¼‰
    if get("flexibility") >= 3.5 and get("cognitive_style") < 3.5:
        all_badges.append({
            "id": "flexible_shifter",
            "name": "æŸ”è»Ÿã‚·ãƒ•ã‚¿ãƒ¼",
            "emoji": "ğŸ”„",
            "category": "æ€è€ƒã‚¹ã‚¿ã‚¤ãƒ«",
            "rarity": 1,
            "description": "çŠ¶æ³ã«å¿œã˜ã¦è€ƒãˆã‚’å¤‰ãˆã‚‰ã‚Œã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ========== ã‚«ãƒ†ã‚´ãƒª4: ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹æ€§ç³» ==========

    # ğŸ›¡ï¸ é‹¼ã®ãƒ¡ãƒ³ã‚¿ãƒ«ï¼ˆâ˜…â˜…â˜…ï¼‰
    if get("stress_tolerance") >= 3.5 and get("emotional_stability") >= 3.5 and get("stress_recovery") >= 3.5:
        all_badges.append({
            "id": "steel_mental",
            "name": "é‹¼ã®ãƒ¡ãƒ³ã‚¿ãƒ«",
            "emoji": "ğŸ›¡ï¸",
            "category": "ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹æ€§",
            "rarity": 3,
            "description": "ã‚¹ãƒˆãƒ¬ã‚¹ã«å¼·ã„ã‚¿ã‚¤ãƒ—"
        })

    # ğŸŒ¸ ç¹Šç´°ã•ã‚“ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("sensitivity") >= 3.5 and get("anxiety_tendency") >= 3.5:
        all_badges.append({
            "id": "sensitive_type",
            "name": "ç¹Šç´°ã•ã‚“",
            "emoji": "ğŸŒ¸",
            "category": "ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹æ€§",
            "rarity": 2,
            "description": "æ„Ÿå—æ€§ãŒè±Šã‹ãªã‚¿ã‚¤ãƒ—"
        })

    # ğŸŒ ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¢ãƒ³ã‚¹ã‚¿ãƒ¼ï¼ˆâ˜…â˜…â˜…ï¼‰
    if get("self_evaluation") >= 3.5 and get("self_efficacy") >= 3.5 and get("anxiety_tendency") < 3.0:
        all_badges.append({
            "id": "positive_monster",
            "name": "ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¢ãƒ³ã‚¹ã‚¿ãƒ¼",
            "emoji": "ğŸŒ",
            "category": "ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹æ€§",
            "rarity": 3,
            "description": "å‰å‘ãã§è‡ªå·±è‚¯å®šæ„ŸãŒé«˜ã„ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ”¥ æˆé•·ãƒãƒ³ã‚°ãƒªãƒ¼ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("growth_motivation") >= 3.5 and get("intrinsic_motivation") >= 3.5:
        all_badges.append({
            "id": "growth_hungry",
            "name": "æˆé•·ãƒãƒ³ã‚°ãƒªãƒ¼",
            "emoji": "ğŸ”¥",
            "category": "ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹æ€§",
            "rarity": 2,
            "description": "è‡ªå·±æˆé•·ã¸ã®æ„æ¬²ãŒå¼·ã„ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ˜Œ å®‰å®šå¿—å‘ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("emotional_stability") >= 3.5 and get("risk_aversion") >= 3.5 and get("calmness") >= 3.5:
        all_badges.append({
            "id": "stability_seeker",
            "name": "å®‰å®šå¿—å‘",
            "emoji": "ğŸ˜Œ",
            "category": "ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹æ€§",
            "rarity": 2,
            "description": "ç©ã‚„ã‹ã§å®‰å®šã‚’å¥½ã‚€ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ¢ åˆºæ¿€ã‚·ãƒ¼ã‚«ãƒ¼ï¼ˆâ˜…â˜…â˜†ï¼‰
    if get("risk_tolerance") >= 3.5 and get("activity_level") >= 3.5 and get("risk_aversion") < 3.0:
        all_badges.append({
            "id": "thrill_seeker",
            "name": "åˆºæ¿€ã‚·ãƒ¼ã‚«ãƒ¼",
            "emoji": "ğŸ¢",
            "category": "ãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹æ€§",
            "rarity": 2,
            "description": "æ–°ã—ã„æŒ‘æˆ¦ãŒå¥½ããªã‚¿ã‚¤ãƒ—"
        })

    # ========== ã‚«ãƒ†ã‚´ãƒª5: ä»•äº‹è¦³ç³» ==========

    # ğŸ† çµæœã«ã‚³ãƒŸãƒƒãƒˆï¼ˆâ˜…â˜†â˜†ï¼‰
    if get("value_process_vs_result") < 3.0 and get("competitiveness") >= 3.5:
        all_badges.append({
            "id": "result_oriented",
            "name": "çµæœã«ã‚³ãƒŸãƒƒãƒˆ",
            "emoji": "ğŸ†",
            "category": "ä»•äº‹è¦³",
            "rarity": 1,
            "description": "çµæœã‚’é‡è¦–ã™ã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ğŸŒ± ãƒ—ãƒ­ã‚»ã‚¹é‡è¦–ï¼ˆâ˜…â˜†â˜†ï¼‰
    if get("value_process_vs_result") >= 3.5:
        all_badges.append({
            "id": "process_oriented",
            "name": "ãƒ—ãƒ­ã‚»ã‚¹é‡è¦–",
            "emoji": "ğŸŒ±",
            "category": "ä»•äº‹è¦³",
            "rarity": 1,
            "description": "éç¨‹ã‚’å¤§åˆ‡ã«ã™ã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # âš–ï¸ ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚µãƒ¼ï¼ˆâ˜…â˜†â˜†ï¼‰
    if get("work_life_value") >= 3.5 and get("work_life_boundary") >= 3.5:
        all_badges.append({
            "id": "work_life_balancer",
            "name": "ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚µãƒ¼",
            "emoji": "âš–ï¸",
            "category": "ä»•äº‹è¦³",
            "rarity": 1,
            "description": "ä»•äº‹ã¨ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚’ä¸¡ç«‹ã™ã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ğŸ’¼ ä»•äº‹äººé–“ï¼ˆâ˜…â˜†â˜†ï¼‰
    if get("work_life_value") < 3.0 and get("external_motivation") >= 3.5:
        all_badges.append({
            "id": "workaholic",
            "name": "ä»•äº‹äººé–“",
            "emoji": "ğŸ’¼",
            "category": "ä»•äº‹è¦³",
            "rarity": 1,
            "description": "ä»•äº‹ã«å…¨åŠ›æŠ•çƒã‚¿ã‚¤ãƒ—"
        })

    # ğŸŒŸ è‡ªå·±å®Ÿç¾ã‚¿ã‚¤ãƒ—ï¼ˆâ˜…â˜†â˜†ï¼‰
    if get("intrinsic_motivation") >= 3.5 and get("value_self_vs_world") >= 3.5:
        all_badges.append({
            "id": "self_actualization",
            "name": "è‡ªå·±å®Ÿç¾ã‚¿ã‚¤ãƒ—",
            "emoji": "ğŸŒŸ",
            "category": "ä»•äº‹è¦³",
            "rarity": 1,
            "description": "è‡ªåˆ†ã‚‰ã—ã•ã‚’è¿½æ±‚ã™ã‚‹ã‚¿ã‚¤ãƒ—"
        })

    # ========== ã‚«ãƒ†ã‚´ãƒª6: ç‰¹æ®Šãƒãƒƒã‚¸ï¼ˆPCå€¤ãƒ™ãƒ¼ã‚¹ï¼‰==========

    pc1 = pc.get("PC1", 0)
    pc2 = pc.get("PC2", 0)
    pc3 = pc.get("PC3", 0)
    pc4 = pc.get("PC4", 0)

    # ğŸŒˆ ã‚ªãƒ¼ãƒ«ãƒ©ã‚¦ãƒ³ãƒ€ãƒ¼ï¼ˆâ˜…â˜…â˜…ï¼‰- å…¨PCå€¤ãŒãƒãƒ©ãƒ³ã‚¹
    if all(-0.8 <= v <= 0.8 for v in [pc1, pc2, pc3, pc4]):
        all_badges.append({
            "id": "all_rounder",
            "name": "ã‚ªãƒ¼ãƒ«ãƒ©ã‚¦ãƒ³ãƒ€ãƒ¼",
            "emoji": "ğŸŒˆ",
            "category": "ç‰¹æ®Š",
            "rarity": 3,
            "description": "ãƒãƒ©ãƒ³ã‚¹å‹ã®ç¨€æœ‰ãªå­˜åœ¨"
        })

    # â­ æ¥µã¿äººï¼ˆâ˜…â˜…â˜…ï¼‰- ã„ãšã‚Œã‹ã®PCå€¤ãŒçªå‡º
    if any(abs(v) > 1.5 for v in [pc1, pc2, pc3, pc4]):
        all_badges.append({
            "id": "extremist",
            "name": "æ¥µã¿äºº",
            "emoji": "â­",
            "category": "ç‰¹æ®Š",
            "rarity": 3,
            "description": "çªå‡ºã—ãŸç‰¹æ€§ã‚’æŒã¤å­˜åœ¨"
        })

    # ğŸ­ äºŒé¢æ€§ã®æŒã¡ä¸»ï¼ˆâ˜…â˜…â˜…ï¼‰- å¯¾ç…§çš„ãªç‰¹æ€§
    if abs(pc1 - pc2) > 2.0 or abs(pc3 - pc4) > 2.0:
        all_badges.append({
            "id": "dual_nature",
            "name": "äºŒé¢æ€§ã®æŒã¡ä¸»",
            "emoji": "ğŸ­",
            "category": "ç‰¹æ®Š",
            "rarity": 3,
            "description": "å¯¾ç…§çš„ãªç‰¹æ€§ã‚’ä½µã›æŒã¤å­˜åœ¨"
        })

    # ğŸ’ ãƒ€ã‚¤ãƒ¤ã®åŸçŸ³ï¼ˆâ˜…â˜…â˜…ï¼‰- ãƒªãƒ¼ãƒ€ãƒ¼ç´ è³ªã‚ã‚‹ãŒè‡ªè¦šãªã—
    if pc4 >= 1.0 and get("self_efficacy") < 3.5:
        all_badges.append({
            "id": "hidden_gem",
            "name": "ãƒ€ã‚¤ãƒ¤ã®åŸçŸ³",
            "emoji": "ğŸ’",
            "category": "ç‰¹æ®Š",
            "rarity": 3,
            "description": "ãƒªãƒ¼ãƒ€ãƒ¼ç´ è³ªãŒã‚ã‚‹ãŒè‡ªè¦šãªã—ã‚¿ã‚¤ãƒ—"
        })

    # ========== ãƒãƒƒã‚¸é¸å‡º ==========

    # ãƒ¬ã‚¢åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
    all_badges.sort(key=lambda x: x["rarity"], reverse=True)

    # åŒã‚«ãƒ†ã‚´ãƒªã‹ã‚‰2å€‹ä»¥ä¸Šã¯å‡ºã•ãªã„
    selected_badges = []
    category_count = {}

    for badge in all_badges:
        cat = badge["category"]
        if category_count.get(cat, 0) < 2:
            selected_badges.append(badge)
            category_count[cat] = category_count.get(cat, 0) + 1

        if len(selected_badges) >= 5:
            break

    # ãƒ¬ã‚¢åº¦ã‚’æ˜Ÿè¡¨ç¤ºã«å¤‰æ›
    rarity_display = {1: "â˜…â˜†â˜†", 2: "â˜…â˜…â˜†", 3: "â˜…â˜…â˜…"}
    for badge in selected_badges:
        badge["rarity_display"] = rarity_display.get(badge["rarity"], "â˜…â˜†â˜†")

    return selected_badges


# =========================
# è©³ç´°è§£èª¬æ–‡ç”Ÿæˆï¼ˆ200æ–‡å­—ç¨‹åº¦ã€ãƒãƒ¼ãƒŠãƒ åŠ¹æœï¼‰
# =========================
def generate_detailed_description(pc: dict, type_label: str, features_55: dict = None) -> str:
    """
    200æ–‡å­—ç¨‹åº¦ã®è©³ç´°ãªæ€§æ ¼è§£èª¬ã‚’ç”Ÿæˆ
    ãƒãƒ¼ãƒŠãƒ åŠ¹æœã‚’æ„è­˜ã—ãŸæŸ”ã‚‰ã‹ã„è¡¨ç¾
    """
    pc1, pc2, pc3, pc4 = pc.get("PC1", 0), pc.get("PC2", 0), pc.get("PC3", 0), pc.get("PC4", 0)

    # ã‚¿ã‚¤ãƒ—åˆ¥ãƒ™ãƒ¼ã‚¹æ–‡
    base_texts = {
        "S": "ã‚ãªãŸã¯å‘¨å›²ã®äººã®æ°—æŒã¡ã‚’æ•æ„Ÿã«æ„Ÿã˜å–ã‚Šã€ãƒãƒ¼ãƒ ã®èª¿å’Œã‚’è‡ªç„¶ã¨æ„è­˜ã§ãã‚‹æ–¹ã§ã™ã€‚",
        "P": "ã‚ãªãŸã¯äººã¨ã®é–¢ã‚ã‚Šã®ä¸­ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å¾—ã‚‰ã‚Œã‚‹ã€ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³è±Šã‹ãªæ–¹ã§ã™ã€‚",
        "C": "ã‚ãªãŸã¯ç‰©äº‹ã‚’è«–ç†çš„ã«åˆ†æã—ã€ç€å®Ÿã«æˆæœã‚’ç©ã¿ä¸Šã’ã¦ã„ãã“ã¨ãŒã§ãã‚‹æ–¹ã§ã™ã€‚",
        "D": "ã‚ãªãŸã¯ç›®æ¨™ã«å‘ã‹ã£ã¦è‡ªã‚‰é“ã‚’åˆ‡ã‚Šé–‹ãã€å‘¨å›²ã‚’å·»ãè¾¼ã‚“ã§ã„ãåŠ›ã‚’ãŠæŒã¡ã®æ–¹ã§ã™ã€‚",
    }

    # è£œè¶³æ–‡ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
    supplements = []

    if pc1 > 0.3:
        supplements.append("äººã®å½¹ã«ç«‹ã¡ãŸã„ã¨ã„ã†æ°—æŒã¡ãŒå¼·ãã€å›°ã£ã¦ã„ã‚‹äººã‚’è¦‹éã”ã›ãªã„å„ªã—ã•ãŒã‚ã‚Šã¾ã™")
    elif pc1 < -0.3:
        supplements.append("è‡ªåˆ†ã®ä¿¡å¿µã‚’å¤§åˆ‡ã«ã—ãªãŒã‚‰ã€ã¶ã‚Œãªã„è»¸ã‚’æŒã£ã¦è¡Œå‹•ã§ãã¾ã™")
    else:
        supplements.append("çŠ¶æ³ã«å¿œã˜ã¦æŸ”è»Ÿã«å½¹å‰²ã‚’å¤‰ãˆã‚‰ã‚Œã‚‹é©å¿œåŠ›ãŒã‚ã‚Šã¾ã™")

    if pc2 > 0.3:
        supplements.append("åˆå¯¾é¢ã®äººã¨ã‚‚æ‰“ã¡è§£ã‘ã‚„ã™ãã€å ´ã‚’å’Œã¾ã›ã‚‹åŠ›ãŒã‚ã‚Šã¾ã™")
    elif pc2 < -0.3:
        supplements.append("ä¸€äººã®æ™‚é–“ã‚’å¤§åˆ‡ã«ã—ã€æ·±ãè€ƒãˆã‚‹ã“ã¨ã§è‰¯ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç”Ÿã¿å‡ºã›ã¾ã™")

    if pc3 > 0.3:
        supplements.append("ç´°éƒ¨ã¾ã§æ°—ã‚’é…ã‚Šã€ãƒŸã‚¹ã‚’æœªç„¶ã«é˜²ãæ³¨æ„åŠ›ãŒã‚ã‚Šã¾ã™")
    elif pc3 < -0.3:
        supplements.append("ç›´æ„Ÿã‚’ä¿¡ã˜ã¦ç´ æ—©ãè¡Œå‹•ã«ç§»ã›ã‚‹æ±ºæ–­åŠ›ãŒã‚ã‚Šã¾ã™")

    if pc4 > 0.3:
        supplements.append("æ–°ã—ã„ã“ã¨ã«æŒ‘æˆ¦ã™ã‚‹å‹‡æ°—ã¨ã€ãã‚Œã‚’å®Ÿç¾ã•ã›ã‚‹è¡Œå‹•åŠ›ãŒã‚ã‚Šã¾ã™")
    elif pc4 < -0.3:
        supplements.append("ç¸ã®ä¸‹ã®åŠ›æŒã¡ã¨ã—ã¦ã€ãƒãƒ¼ãƒ ã‚’æ”¯ãˆã‚‹é ¼ã‚‚ã—ã„å­˜åœ¨ã§ã™")

    # ãƒãƒ¼ãƒŠãƒ åŠ¹æœã‚’æ„è­˜ã—ãŸæ™®éçš„æ–‡
    universal = "æ™‚ã«è‡ªåˆ†ã®èƒ½åŠ›ã‚’éå°è©•ä¾¡ã—ã¦ã—ã¾ã†ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ãŒã€å®Ÿéš›ã«ã¯å‘¨å›²ã‹ã‚‰ä¿¡é ¼ã•ã‚Œã¦ã„ã‚‹å ´é¢ã‚‚å¤šã„ã¯ãšã§ã™ã€‚è‡ªåˆ†ã‚‰ã—ã•ã‚’å¤§åˆ‡ã«ã—ãªãŒã‚‰ã€å¾—æ„ãªã“ã¨ã‚’ä¼¸ã°ã—ã¦ã„ãã“ã¨ã§ã€ã•ã‚‰ã«è¼ã‘ã‚‹ã§ã—ã‚‡ã†ã€‚"

    base = base_texts.get(type_label, base_texts["S"])
    supplement_text = "ã€‚".join(supplements[:2]) + "ã€‚" if supplements else ""

    full_text = base + supplement_text + universal

    # 200æ–‡å­—ç¨‹åº¦ã«èª¿æ•´
    if len(full_text) > 250:
        full_text = full_text[:247] + "..."

    return full_text


# =========================
# ä¸€è¨€ã¾ã¨ã‚ç”Ÿæˆ
# =========================
def generate_one_liner(pc: dict) -> str:
    """PCã‚¹ã‚³ã‚¢ã‹ã‚‰ä¸€è¨€ã¾ã¨ã‚ã‚’ç”Ÿæˆ"""
    pc1, pc2, pc3, pc4 = pc["PC1"], pc["PC2"], pc["PC3"], pc["PC4"]

    traits = []

    # é«˜ã„å‚¾å‘ã‚’ç‰¹å®š
    if pc1 > 0.4:
        traits.append("å‘¨å›²ã¸ã®æ°—é…ã‚ŠãŒã§ãã‚‹")
    if pc2 > 0.4:
        traits.append("äººã¨æ¥ã™ã‚‹ã“ã¨ãŒå¥½ã")
    if pc3 > 0.4:
        traits.append("è«–ç†çš„ãªè€ƒãˆæ–¹ã‚’é‡ã‚“ã˜ã‚‹")
    if pc4 > 0.4:
        traits.append("è‡ªã‚‰ç‡å…ˆã—ã¦å‹•ã")

    # ä½ã„å‚¾å‘ã‚‚åŠ å‘³
    if pc1 < -0.4:
        traits.append("è‡ªåˆ†ã®ãƒšãƒ¼ã‚¹ã‚’å¤§åˆ‡ã«ã™ã‚‹")
    if pc2 < -0.4:
        traits.append("ã˜ã£ãã‚Šè€ƒãˆã¦ã‹ã‚‰è¡Œå‹•ã™ã‚‹")
    if pc3 < -0.4:
        traits.append("ç›´æ„Ÿã‚’å¤§åˆ‡ã«ã™ã‚‹")
    if pc4 < -0.4:
        traits.append("ã‚µãƒãƒ¼ãƒˆå½¹ã¨ã—ã¦åŠ›ã‚’ç™ºæ®ã™ã‚‹")

    if not traits:
        # ãƒãƒ©ãƒ³ã‚¹å‹ï¼šå¾®ç´°ãªå·®ç•°ã‹ã‚‰ç‰¹å¾´ã‚’æŠ½å‡º
        return generate_balanced_type_one_liner(pc1, pc2, pc3, pc4)

    if len(traits) == 1:
        return f"{traits[0]}ã‚¿ã‚¤ãƒ—ã§ã™"
    elif len(traits) == 2:
        return f"{traits[0]}ã‚¿ã‚¤ãƒ—ã§ã€{traits[1]}å‚¾å‘ãŒã‚ã‚Šã¾ã™"
    else:
        return f"{traits[0]}ã‚¿ã‚¤ãƒ—ã§ã€{traits[1]}å‚¾å‘ãŒã‚ã‚Šã€{traits[2]}ç‰¹å¾´ãŒã‚ã‚Šã¾ã™"


def generate_balanced_type_one_liner(pc1: float, pc2: float, pc3: float, pc4: float) -> str:
    """ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆå…¨ã¦ã®å€¤ãŒÂ±0.4ä»¥å†…ï¼‰ã®å ´åˆã€å¾®ç´°ãªå·®ç•°ã‹ã‚‰ç‰¹å¾´ã‚’æŠ½å‡º"""
    pc_values = {"å’Œ": pc1, "é™½": pc2, "ç†": pc3, "å°": pc4}

    # æœ€ã‚‚é«˜ã„å€¤ã¨æœ€ã‚‚ä½ã„å€¤ã‚’å–å¾—
    sorted_pcs = sorted(pc_values.items(), key=lambda x: x[1], reverse=True)
    highest = sorted_pcs[0]
    second_highest = sorted_pcs[1]
    lowest = sorted_pcs[-1]

    # å¾®ç´°ãªå‚¾å‘ã‚’è¡¨ç¾
    trait_descriptions = {
        "å’Œ": "å”èª¿æ€§ã‚’æ„è­˜ã—ã¤ã¤",
        "é™½": "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¤§åˆ‡ã«ã—ãªãŒã‚‰",
        "ç†": "ç‰©äº‹ã‚’æ•´ç†ã—ã¦è€ƒãˆã¤ã¤",
        "å°": "ä¸»ä½“æ€§ã‚’æŒã£ã¦",
    }

    balance_phrases = [
        "ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸä¸‡èƒ½å‹ã§ã€çŠ¶æ³ã«å¿œã˜ã¦æŸ”è»Ÿã«å½¹å‰²ã‚’å¤‰ãˆã‚‰ã‚Œã¾ã™",
        "ç‰¹å®šã®åã‚ŠãŒãªãã€ã©ã‚“ãªå ´é¢ã«ã‚‚é©å¿œã§ãã‚‹æŸ”è»Ÿæ€§ãŒã‚ã‚Šã¾ã™",
        "4ã¤ã®ç‰¹æ€§ã‚’çŠ¶æ³ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹ã€é©å¿œåŠ›ã®é«˜ã„ã‚¿ã‚¤ãƒ—ã§ã™",
    ]

    # ç›¸å¯¾çš„ãªå‚¾å‘ã‚’æ–‡ç« ã«
    if highest[1] - lowest[1] > 0.2:  # ã‚ãšã‹ã§ã‚‚å·®ãŒã‚ã‚‹å ´åˆ
        return f"{trait_descriptions[highest[0]]}å…¨ä½“çš„ã«ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæŸ”è»Ÿãªã‚¿ã‚¤ãƒ—ã§ã™"
    else:
        # å®Œå…¨ã«ãƒ•ãƒ©ãƒƒãƒˆãªå ´åˆ
        import random
        return random.choice(balance_phrases)


# =========================
# å¼·ã¿ãƒ»å¼±ã¿ç”Ÿæˆ
# =========================
def generate_strengths_weaknesses(pc: dict) -> dict:
    """PCã‚¹ã‚³ã‚¢ã‹ã‚‰å¼·ã¿ãƒ»å¼±ã¿ã‚’ç”Ÿæˆ"""
    pc1, pc2, pc3, pc4 = pc["PC1"], pc["PC2"], pc["PC3"], pc["PC4"]
    
    strengths = []
    weaknesses = []
    
    # PC1ï¼ˆå’Œå‹ï¼‰
    if pc1 > 0.4 and pc4 > 0.4:
        strengths.append("å‘¨ã‚Šã‚’å·»ãè¾¼ã¿ãªãŒã‚‰å¼•ã£å¼µã£ã¦ã„ã‘ã‚‹")
    elif pc1 > 0.4:
        strengths.append("ãƒãƒ¼ãƒ ã®é›°å›²æ°—ã‚’å’Œã‚‰ã’ã€ãƒ¡ãƒ³ãƒãƒ¼ã‚’æ”¯ãˆã‚‹åŠ›ãŒã‚ã‚‹")
    
    if pc1 < -0.4:
        weaknesses.append("å‘¨å›²ã¸ã®é…æ…®ãŒå¾Œå›ã—ã«ãªã‚Šã‚„ã™ã„å ´é¢ãŒã‚ã‚‹")
    
    # PC2ï¼ˆé™½å‹ï¼‰
    if pc2 > 0.4:
        strengths.append("ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³åŠ›ãŒé«˜ãã€å ´ã‚’æ˜ã‚‹ãã§ãã‚‹")
    if pc2 < -0.4:
        weaknesses.append("è‡ªå·±ä¸»å¼µãŒæ§ãˆã‚ã§ã€ç™ºè¨€ã‚’é æ…®ã—ãŒã¡")
    
    # PC3ï¼ˆç†å‹ï¼‰
    if pc3 > 0.4:
        strengths.append("è«–ç†çš„ã«ç‰©äº‹ã‚’æ•´ç†ã—ã€ãƒªã‚¹ã‚¯ã‚’è¦‹æ¥µã‚ã‚‰ã‚Œã‚‹")
    if pc3 < -0.4:
        weaknesses.append("è¡Œå‹•ãŒæ—©ã„åˆ†ã€ä¸å¯§ã•ãŒä¸è¶³ã™ã‚‹å ´é¢ãŒã‚ã‚‹")
    
    # PC4ï¼ˆå°å‹ï¼‰
    if pc4 > 0.4:
        strengths.append("ä¸»ä½“çš„ã«å‹•ãã€ãƒãƒ¼ãƒ ã‚’ç‰½å¼•ã™ã‚‹åŠ›ãŒã‚ã‚‹")
    if pc4 < -0.4:
        weaknesses.append("è‡ªåˆ†ã‹ã‚‰å‰ã«å‡ºã‚‹ã“ã¨ã‚’é¿ã‘ãŒã¡")
    
    # çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³
    if pc1 > 0.4 and pc2 < -0.4:
        weaknesses.append("ã‚µãƒãƒ¼ãƒˆã«å›ã‚Šã™ãã¦è² æ‹…ã‚’æŠ±ãˆè¾¼ã¿ã‚„ã™ã„")
    
    if pc3 < -0.4 and pc4 > 0.4:
        weaknesses.append("ã‚¹ãƒ”ãƒ¼ãƒ‰é‡è¦–ã§ç´°éƒ¨ã®ç¢ºèªãŒç”˜ããªã‚‹ã“ã¨ãŒã‚ã‚‹")
    
    # ãƒãƒ©ãƒ³ã‚¹å‹ã®å ´åˆï¼šå¾®ç´°ãªå·®ç•°ã‹ã‚‰å¼·ã¿ãƒ»å¼±ã¿ã‚’ç”Ÿæˆ
    if not strengths or not weaknesses:
        balanced_strengths, balanced_weaknesses = generate_balanced_type_sw(pc1, pc2, pc3, pc4)
        if not strengths:
            strengths = balanced_strengths
        if not weaknesses:
            weaknesses = balanced_weaknesses

    return {
        "strengths": strengths,
        "weaknesses": weaknesses
    }


def generate_balanced_type_sw(pc1: float, pc2: float, pc3: float, pc4: float) -> tuple:
    """ãƒãƒ©ãƒ³ã‚¹å‹ã®å¼·ã¿ãƒ»å¼±ã¿ã‚’å¾®ç´°ãªå·®ç•°ã‹ã‚‰ç”Ÿæˆ"""
    pc_values = [("å’Œ", pc1), ("é™½", pc2), ("ç†", pc3), ("å°", pc4)]

    # ç›¸å¯¾çš„ã«é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_pcs = sorted(pc_values, key=lambda x: x[1], reverse=True)
    relative_high = sorted_pcs[0][0]
    relative_low = sorted_pcs[-1][0]

    strengths = []
    weaknesses = []

    # ãƒãƒ©ãƒ³ã‚¹å‹ã®åŸºæœ¬çš„ãªå¼·ã¿
    strengths.append("çŠ¶æ³ã«å¿œã˜ã¦æŸ”è»Ÿã«å¯¾å¿œã§ãã‚‹ãƒãƒ©ãƒ³ã‚¹åŠ›ãŒã‚ã‚‹")
    strengths.append("ç‰¹å®šã®å½¹å‰²ã«ç¸›ã‚‰ã‚Œãšã€æ§˜ã€…ãªå ´é¢ã§åŠ›ã‚’ç™ºæ®ã§ãã‚‹")

    # ç›¸å¯¾çš„ã«é«˜ã„å‚¾å‘ã‹ã‚‰è¿½åŠ ã®å¼·ã¿
    high_traits = {
        "å’Œ": "å‘¨å›²ã¨ã®èª¿å’Œã‚’è‡ªç„¶ã¨æ„è­˜ã§ãã‚‹",
        "é™½": "å¿…è¦ã«å¿œã˜ã¦ç™ºè¨€åŠ›ã‚’ç™ºæ®ã§ãã‚‹",
        "ç†": "å†·é™ã«çŠ¶æ³ã‚’åˆ†æã™ã‚‹è¦–ç‚¹ã‚’æŒã¦ã‚‹",
        "å°": "å¿…è¦ãªæ™‚ã«ã¯å‰ã«å‡ºã‚‹å‹‡æ°—ãŒã‚ã‚‹",
    }
    strengths.append(high_traits.get(relative_high, "å¤šè§’çš„ãªè¦–ç‚¹ã§ç‰©äº‹ã‚’æ‰ãˆã‚‰ã‚Œã‚‹"))

    # ãƒãƒ©ãƒ³ã‚¹å‹ç‰¹æœ‰ã®å¼±ã¿
    weaknesses.append("ã©ã®å½¹å‰²ã‚’æ‹…ã†ã‹è¿·ã†ã“ã¨ãŒã‚ã‚‹")

    # ç›¸å¯¾çš„ã«ä½ã„å‚¾å‘ã‹ã‚‰å¼±ã¿
    low_traits = {
        "å’Œ": "ãƒãƒ¼ãƒ ã¸ã®æ°—é…ã‚Šã‚’ã‚‚ã†å°‘ã—æ„è­˜ã™ã‚‹ã¨è‰¯ã„ã‹ã‚‚",
        "é™½": "ã‚‚ã†å°‘ã—ç©æ¥µçš„ã«ç™ºè¨€ã—ã¦ã‚‚è‰¯ã„å ´é¢ãŒã‚ã‚‹",
        "ç†": "æ™‚ã«ç«‹ã¡æ­¢ã¾ã£ã¦ç¢ºèªã™ã‚‹ç¿’æ…£ãŒã‚ã‚‹ã¨å®‰å¿ƒ",
        "å°": "è‡ªåˆ†ã‹ã‚‰ææ¡ˆã™ã‚‹æ©Ÿä¼šã‚’å¢—ã‚„ã—ã¦ã‚‚è‰¯ã„",
    }
    weaknesses.append(low_traits.get(relative_low, "ç‰¹å®šã®å¼·ã¿ã‚’ä¼¸ã°ã™ã“ã¨ã§æ›´ã«è¼ã‘ã‚‹"))

    return strengths, weaknesses


# =========================
# ã‚¹ãƒˆãƒ¬ã‚¹å ´é¢ãƒ»å¯¾å‡¦æ³•ç”Ÿæˆ
# =========================
def generate_stress_info(pc: dict, features_55: dict) -> dict:
    """ã‚¹ãƒˆãƒ¬ã‚¹ã‚’æ„Ÿã˜ã‚„ã™ã„å ´é¢ã¨å¯¾å‡¦æ³•ã‚’ç”Ÿæˆ"""
    pc1, pc2, pc3, pc4 = pc["PC1"], pc["PC2"], pc["PC3"], pc["PC4"]
    
    stress_situations = []
    stress_coping = []
    
    # PC1ï¼ˆå’Œå‹ï¼‰
    if pc1 > 0.4:
        stress_situations.append("å¯¾ç«‹ã‚„è¡çªãŒç¶šãç’°å¢ƒ")
        stress_coping.append("ä¿¡é ¼ã§ãã‚‹äººã¨çŠ¶æ³ã‚’å…±æœ‰ã™ã‚‹")
    elif pc1 < -0.4:
        stress_situations.append("éåº¦ãªå”èª¿ã‚’æ±‚ã‚ã‚‰ã‚Œã‚‹å ´é¢")
        stress_coping.append("ä¸€äººã§è€ƒãˆã‚‹æ™‚é–“ã‚’ç¢ºä¿ã™ã‚‹")
    
    # PC2ï¼ˆé™½å‹ï¼‰
    if pc2 > 0.4:
        stress_situations.append("å­¤ç«‹ã—ãŸç’°å¢ƒã§ã®é•·æ™‚é–“ä½œæ¥­")
        stress_coping.append("é©åº¦ã«äººã¨è©±ã™æ©Ÿä¼šã‚’ä½œã‚‹")
    elif pc2 < -0.4:
        stress_situations.append("å¤§äººæ•°ã®å‰ã§ã®ç™ºè¡¨ã‚„æ³¨ç›®ã‚’æµ´ã³ã‚‹å ´é¢")
        stress_coping.append("è½ã¡ç€ã‘ã‚‹é™ã‹ãªå ´æ‰€ã§æ•´ç†ã™ã‚‹")
    
    # PC3ï¼ˆç†å‹ï¼‰
    if pc3 > 0.4:
        stress_situations.append("æ˜ç¢ºãªæŒ‡ç¤ºãŒãªãæ›–æ˜§ãªçŠ¶æ³")
        stress_coping.append("æƒ…å ±ã‚’æ•´ç†ã—ã¦ãƒªã‚¹ãƒˆåŒ–ã™ã‚‹")
    elif pc3 < -0.4:
        stress_situations.append("ç´°ã‹ã„æ‰‹é †ã‚„ãƒ«ãƒ¼ãƒ«ã«ç¸›ã‚‰ã‚Œã‚‹ç’°å¢ƒ")
        stress_coping.append("è‡ªåˆ†ãªã‚Šã®ã‚„ã‚Šæ–¹ã‚’è¦‹ã¤ã‘ã‚‹ä½™åœ°ã‚’ç¢ºä¿ã™ã‚‹")
    
    # PC4ï¼ˆå°å‹ï¼‰
    if pc4 > 0.4:
        stress_situations.append("æ±ºå®šæ¨©ãŒãªãæŒ‡ç¤ºå¾…ã¡ã®çŠ¶æ³")
        stress_coping.append("å°ã•ãªç¯„å›²ã§ã‚‚ä¸»ä½“çš„ã«å‹•ã‘ã‚‹éƒ¨åˆ†ã‚’è¦‹ã¤ã‘ã‚‹")
    elif pc4 < -0.4:
        stress_situations.append("ãƒªãƒ¼ãƒ€ãƒ¼å½¹ã‚’çªç„¶ä»»ã•ã‚Œã‚‹å ´é¢")
        stress_coping.append("å‘¨å›²ã®ã‚µãƒãƒ¼ãƒˆã‚’ç©æ¥µçš„ã«æ±‚ã‚ã‚‹")
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    if not stress_situations:
        stress_situations.append("éåº¦ãªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ãŒã‹ã‹ã‚‹çŠ¶æ³")
    
    if not stress_coping:
        stress_coping.append("è‡ªåˆ†ã«åˆã£ãŸãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥æ–¹æ³•ã‚’è¦‹ã¤ã‘ã‚‹")
    
    return {
        "stress_situations": stress_situations,
        "stress_coping": stress_coping
    }


# =========================
# ä»•äº‹ã‚¹ã‚¿ã‚¤ãƒ«ç”Ÿæˆ
# =========================
def generate_work_style(pc: dict) -> dict:
    """ä»•äº‹ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
    pc1, pc2, pc3, pc4 = pc["PC1"], pc["PC2"], pc["PC3"], pc["PC4"]
    
    work_style = {
        "collaboration": "",  # äººã¨é€²ã‚ã‚‹ or ä¸€äººã§åŠ›ã‚’ç™ºæ®
        "pace": "",           # ã‚¹ãƒ”ãƒ¼ãƒ‰å‹ or æ…é‡å‹
        "approach": "",       # ä¸»ä½“å‹ or ã‚µãƒãƒ¼ãƒˆå‹
        "thinking": "",       # è«–ç†å‹ or ç›´æ„Ÿå‹
    }
    
    # å”åƒã‚¹ã‚¿ã‚¤ãƒ«
    if pc2 > 0.3:
        work_style["collaboration"] = "äººã¨ä¸€ç·’ã«é€²ã‚ã‚‹ã“ã¨ã§åŠ›ã‚’ç™ºæ®ã—ã‚„ã™ã„"
    elif pc2 < -0.3:
        work_style["collaboration"] = "ä¸€äººã§é›†ä¸­ã—ã¦å–ã‚Šçµ„ã‚€ã“ã¨ã§åŠ›ã‚’ç™ºæ®ã—ã‚„ã™ã„"
    else:
        work_style["collaboration"] = "çŠ¶æ³ã«å¿œã˜ã¦å”åƒã‚‚å€‹äººä½œæ¥­ã‚‚ã“ãªã›ã‚‹"
    
    # ãƒšãƒ¼ã‚¹
    if pc3 > 0.3:
        work_style["pace"] = "æ…é‡ã«ç¢ºèªã—ãªãŒã‚‰é€²ã‚ã‚‹æ…é‡å‹"
    elif pc3 < -0.3:
        work_style["pace"] = "ç´ æ—©ãè¡Œå‹•ã«ç§»ã™ã‚¹ãƒ”ãƒ¼ãƒ‰å‹"
    else:
        work_style["pace"] = "å ´é¢ã«å¿œã˜ã¦ãƒšãƒ¼ã‚¹ã‚’èª¿æ•´ã§ãã‚‹ã‚¿ã‚¤ãƒ—"
    
    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
    if pc4 > 0.3:
        work_style["approach"] = "è‡ªã‚‰ç‡å…ˆã—ã¦å‹•ãä¸»ä½“å‹"
    elif pc4 < -0.3:
        work_style["approach"] = "ãƒãƒ¼ãƒ ã‚’æ”¯ãˆã‚‹ã‚µãƒãƒ¼ãƒˆå‹"
    else:
        work_style["approach"] = "çŠ¶æ³ã«å¿œã˜ã¦ãƒªãƒ¼ãƒ‰ã‚‚ã‚µãƒãƒ¼ãƒˆã‚‚ã§ãã‚‹ã‚¿ã‚¤ãƒ—"
    
    # æ€è€ƒã‚¹ã‚¿ã‚¤ãƒ«
    if pc3 > 0.3:
        work_style["thinking"] = "ãƒ‡ãƒ¼ã‚¿ã‚„æ ¹æ‹ ã‚’é‡è¦–ã™ã‚‹è«–ç†å‹"
    elif pc3 < -0.3:
        work_style["thinking"] = "ç›´æ„Ÿã‚„æ„Ÿè¦šã‚’å¤§åˆ‡ã«ã™ã‚‹ç›´æ„Ÿå‹"
    else:
        work_style["thinking"] = "è«–ç†ã¨ç›´æ„Ÿã®ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚ŒãŸã‚¿ã‚¤ãƒ—"
    
    return work_style


# =========================
# æ¡ç”¨æ‹…å½“å‘ã‘æƒ…å ±ç”Ÿæˆ
# =========================
def generate_hr_insights(pc: dict, features_55: dict, stress_tolerance: int) -> dict:
    """æ¡ç”¨æ‹…å½“å‘ã‘ã®è©³ç´°æƒ…å ±ã‚’ç”Ÿæˆ"""
    pc1, pc2, pc3, pc4 = pc["PC1"], pc["PC2"], pc["PC3"], pc["PC4"]
    
    # è‰¯ã„æ¥ã—æ–¹ãƒ»æ‚ªã„æ¥ã—æ–¹
    good_approach = []
    bad_approach = []
    
    if pc1 > 0.4:
        good_approach.append("æ„Ÿè¬ã®è¨€è‘‰ã‚’ã“ã¾ã‚ã«ä¼ãˆã‚‹")
        bad_approach.append("ãƒãƒ¼ãƒ ã®å’Œã‚’ä¹±ã™ã‚ˆã†ãªæŒ‡ç¤ºã‚’å‡ºã™")
    elif pc1 < -0.4:
        good_approach.append("å€‹äººã®è£é‡ã‚’å°Šé‡ã™ã‚‹")
        bad_approach.append("éåº¦ãªå”èª¿ã‚’å¼·åˆ¶ã™ã‚‹")
    
    if pc2 > 0.4:
        good_approach.append("æ„è¦‹ã‚’èãæ©Ÿä¼šã‚’å¤šãè¨­ã‘ã‚‹")
        bad_approach.append("é•·æœŸé–“å­¤ç«‹ã•ã›ã‚‹æ¥­å‹™ã‚’ä¸ãˆã‚‹")
    elif pc2 < -0.4:
        good_approach.append("äº‹å‰ã«æº–å‚™æ™‚é–“ã‚’ä¸ãˆã¦ã‹ã‚‰ç™ºè¨€ã‚’æ±‚ã‚ã‚‹")
        bad_approach.append("æ€¥ã«å¤§å‹¢ã®å‰ã§ç™ºè¡¨ã•ã›ã‚‹")
    
    if pc3 > 0.4:
        good_approach.append("æ˜ç¢ºãªåŸºæº–ã‚„æ‰‹é †ã‚’ç¤ºã™")
        bad_approach.append("æ›–æ˜§ãªæŒ‡ç¤ºã§ä¸¸æŠ•ã’ã™ã‚‹")
    elif pc3 < -0.4:
        good_approach.append("å¤§ã¾ã‹ãªæ–¹å‘æ€§ã‚’ç¤ºã—ã¦ä»»ã›ã‚‹")
        bad_approach.append("ç´°ã‹ã„ãƒ«ãƒ¼ãƒ«ã§ç¸›ã‚Šã™ãã‚‹")
    
    if pc4 > 0.4:
        good_approach.append("è£é‡æ¨©ã‚’æŒãŸã›ã¦ä»»ã›ã‚‹")
        bad_approach.append("å…¨ã¦ã®æ±ºå®šã‚’ä¸Šã‹ã‚‰æŠ¼ã—ä»˜ã‘ã‚‹")
    elif pc4 < -0.4:
        good_approach.append("å…·ä½“çš„ãªæŒ‡ç¤ºã¨ã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã™ã‚‹")
        bad_approach.append("çªç„¶ãƒªãƒ¼ãƒ€ãƒ¼å½¹ã‚’ä»»ã›ã‚‹")
    
    # æ°—ã‚’ã¤ã‘ã¦èãã¹ãè³ªå•
    interview_questions = []
    
    if stress_tolerance < 5:
        interview_questions.append("éå»ã«ã‚¹ãƒˆãƒ¬ã‚¹ã‚’æ„Ÿã˜ãŸçµŒé¨“ã¨ã€ã©ã†ä¹—ã‚Šè¶ŠãˆãŸã‹ã‚’å…·ä½“çš„ã«")
    if pc1 < -0.4:
        interview_questions.append("ãƒãƒ¼ãƒ ã§ã®å”åŠ›ãŒå¿…è¦ã ã£ãŸå ´é¢ã§ã®å½¹å‰²ã«ã¤ã„ã¦")
    if pc2 < -0.4:
        interview_questions.append("äººå‰ã§è©±ã™ã“ã¨ã«ã¤ã„ã¦ã©ã†æ„Ÿã˜ã‚‹ã‹")
    if pc4 < -0.4:
        interview_questions.append("è‡ªåˆ†ã‹ã‚‰ææ¡ˆã—ãŸçµŒé¨“ãŒã‚ã‚‹ã‹")
    
    interview_questions.append("å‰è·ï¼ˆã¾ãŸã¯å­¦æ ¡ï¼‰ã§æœ€ã‚‚å›°é›£ã ã£ãŸçŠ¶æ³ã¨å¯¾å‡¦æ³•")
    interview_questions.append("ç†æƒ³ã®ä¸Šå¸åƒã¨ã€ã“ã‚Œã¾ã§ã®ä¸Šå¸ã¨ã®é–¢ä¿‚æ€§")
    
    # è·å ´é©å¿œã«ãŠã‘ã‚‹æ³¨æ„ç‚¹
    adaptation_notes = []
    
    if stress_tolerance < 5:
        adaptation_notes.append("è² è·ãŒé«˜ã„æ™‚æœŸã¯ç‰¹ã«å£°ã‹ã‘ã‚’æ„è­˜ã™ã‚‹")
    if pc1 > 0.4:
        adaptation_notes.append("å‘¨å›²ã¸ã®é…æ…®ã§ç–²å¼Šã—ãªã„ã‚ˆã†ã€é©åº¦ã«ä¼‘æ¯ã‚’ä¿ƒã™")
    if pc2 < -0.4:
        adaptation_notes.append("ç™ºè¨€ã®æ©Ÿä¼šã‚’ç„¡ç†ã«å¢—ã‚„ã•ãšã€å¾ã€…ã«æ…£ã‚Œã•ã›ã‚‹")
    if pc3 > 0.4:
        adaptation_notes.append("å®Œç’§ä¸»ç¾©å‚¾å‘ãŒã‚ã‚Œã°ã€é©åº¦ãªå¦¥å”ç‚¹ã‚’ç¤ºã™")
    if pc4 < -0.4:
        adaptation_notes.append("å°ã•ãªæˆåŠŸä½“é¨“ã‚’ç©ã¾ã›ã¦è‡ªä¿¡ã‚’ã¤ã‘ã•ã›ã‚‹")
    
    # æ—©æœŸé›¢è·ãƒªã‚¹ã‚¯ã‚µã‚¤ãƒ³
    turnover_risks = []
    
    if stress_tolerance < 4:
        turnover_risks.append("è¡¨æƒ…ãŒæš—ããªã‚‹ã€å£æ•°ãŒæ¸›ã‚‹")
    if pc1 > 0.4:
        turnover_risks.append("ãƒãƒ¼ãƒ å†…ã§ã®å­¤ç«‹æ„Ÿã‚’è¨´ãˆã‚‹")
    if pc2 > 0.4:
        turnover_risks.append("ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é¿ã‘ã‚‹ã‚ˆã†ã«ãªã‚‹")
    if pc3 > 0.4:
        turnover_risks.append("ç´°ã‹ã„ãƒŸã‚¹ã‚’éåº¦ã«æ°—ã«ã—å§‹ã‚ã‚‹")
    if pc4 > 0.4:
        turnover_risks.append("ã€Œã‚„ã‚ŠãŒã„ãŒãªã„ã€ã¨æ¼ã‚‰ã™")
    if pc4 < -0.4:
        turnover_risks.append("ã€Œè‡ªåˆ†ã«ã¯å‘ã„ã¦ã„ãªã„ã€ã¨è¨€ã„å§‹ã‚ã‚‹")
    
    turnover_risks.append("é…åˆ»ã‚„æ¬ å‹¤ãŒå¢—ãˆã‚‹")
    turnover_risks.append("å‘¨å›²ã¨ã®ä¼šè©±ãŒæ¸›ã‚‹")
    
    return {
        "good_approach": good_approach[:3],
        "bad_approach": bad_approach[:3],
        "interview_questions": interview_questions[:4],
        "adaptation_notes": adaptation_notes[:4],
        "turnover_risks": turnover_risks[:4],
    }


# =========================
# è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# =========================
def generate_type_report(pc: dict, cluster_id: int, type_label: str, features_55: dict = None) -> dict:
    """PC1ã€œPC4 / ã‚¯ãƒ©ã‚¹ã‚¿ / TYPE ã‚’ã‚‚ã¨ã«ã€è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    pc1 = pc.get("PC1", 0.0)
    pc2 = pc.get("PC2", 0.0)
    pc3 = pc.get("PC3", 0.0)
    pc4 = pc.get("PC4", 0.0)
    
    # å„è»¸ã®ãƒ©ãƒ™ãƒ«ã¨ãƒ¬ãƒ™ãƒ«
    axis_info = []
    for axis_key in ["PC1", "PC2", "PC3", "PC4"]:
        val = pc.get(axis_key, 0.0)
        axis_info.append({
            "key": axis_key,
            "name": AXIS_NAMES_JP[axis_key],
            "description": AXIS_DESCRIPTIONS[axis_key],
            "level": get_level_label(val),
            "level_index": get_level_index(val),
        })
    
    # ä¸€è¨€ã¾ã¨ã‚
    one_liner = generate_one_liner(pc)
    
    # å¼·ã¿ãƒ»å¼±ã¿
    sw = generate_strengths_weaknesses(pc)
    
    # ã‚¹ãƒˆãƒ¬ã‚¹æƒ…å ±
    stress_info = generate_stress_info(pc, features_55 or {})
    
    # ä»•äº‹ã‚¹ã‚¿ã‚¤ãƒ«
    work_style = generate_work_style(pc)
    
    # ã‚¹ãƒˆãƒ¬ã‚¹è€æ€§
    stress_tolerance = calculate_stress_tolerance(features_55 or {}, pc)
    
    # æ¡ç”¨æ‹…å½“å‘ã‘æƒ…å ±
    hr_insights = generate_hr_insights(pc, features_55 or {}, stress_tolerance)

    # è©³ç´°è§£èª¬æ–‡ï¼ˆ200æ–‡å­—ã€ãƒãƒ¼ãƒŠãƒ åŠ¹æœï¼‰
    detailed_description = generate_detailed_description(pc, type_label, features_55)

    # ã‚¿ã‚¤ãƒ—åˆ¥ã‚µãƒãƒªãƒ¼
    type_summary_map = {
        "S": "å’Œå‹ï¼ˆTYPE Sï¼‰ã¨ã—ã¦ã€å‘¨å›²ã¨ã®é–¢ä¿‚æ€§ã‚’å¤§åˆ‡ã«ã—ãªãŒã‚‰ã€ãƒãƒ¼ãƒ å…¨ä½“ã®å®‰å®šã«è²¢çŒ®ã—ã‚„ã™ã„ã‚¿ã‚¤ãƒ—ã§ã™ã€‚",
        "P": "é™½å‹ï¼ˆTYPE Pï¼‰ã¨ã—ã¦ã€äººå‰ã§ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚„æƒ…å ±ç™ºä¿¡ã§åŠ›ã‚’ç™ºæ®ã—ã‚„ã™ã„ã‚¿ã‚¤ãƒ—ã§ã™ã€‚",
        "C": "ç†å‹ï¼ˆTYPE Cï¼‰ã¨ã—ã¦ã€æƒ…å ±ã‚’æ•´ç†ã—ãªãŒã‚‰ãƒªã‚¹ã‚¯ã‚’è¦‹æ¥µã‚ã€å …å®Ÿã«ç‰©äº‹ã‚’é€²ã‚ã‚‹ã‚¿ã‚¤ãƒ—ã§ã™ã€‚",
        "D": "å°å‹ï¼ˆTYPE Dï¼‰ã¨ã—ã¦ã€è‡ªã‚‰æ–¹å‘æ€§ã‚’ç¤ºã—ãªãŒã‚‰ãƒãƒ¼ãƒ ã‚’ç‰½å¼•ã—ã¦ã„ãã‚¿ã‚¤ãƒ—ã§ã™ã€‚",
    }
    summary = type_summary_map.get(
        type_label,
        "4ã¤ã®ç‰¹æ€§ã®ãƒãƒ©ãƒ³ã‚¹ãŒæ¯”è¼ƒçš„ãƒ•ãƒ©ãƒƒãƒˆã§ã€çŠ¶æ³ã«å¿œã˜ã¦æŸ”è»Ÿã«å½¹å‰²ã‚’å¤‰ãˆã‚„ã™ã„ã‚¿ã‚¤ãƒ—ã§ã™ã€‚",
    )

    return {
        "summary": summary,
        "one_liner": one_liner,
        "detailed_description": detailed_description,
        "axis_info": axis_info,
        "strengths": sw["strengths"],
        "weaknesses": sw["weaknesses"],
        "stress_situations": stress_info["stress_situations"],
        "stress_coping": stress_info["stress_coping"],
        "work_style": work_style,
        "stress_tolerance": stress_tolerance,
        "hr_insights": hr_insights,
        "cluster_id": int(cluster_id),
        "type": type_label,
    }


# =========================
# é©è·ãƒãƒƒãƒãƒ³ã‚°æ©Ÿèƒ½
# =========================
def calculate_job_fit(pc: dict, top_n: int = 3) -> list:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®PCã‚¹ã‚³ã‚¢ã¨å„è·ç¨®ã®ç†æƒ³ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¯”è¼ƒã—ã€
    ãƒãƒƒãƒåº¦ã®é«˜ã„é †ã«ãƒ™ã‚¹ãƒˆè·ç¨®ã‚’è¿”ã™
    """
    global job_fit_df

    if job_fit_df is None or job_fit_df.empty:
        return []

    user_pc = np.array([pc.get("PC1", 0), pc.get("PC2", 0), pc.get("PC3", 0), pc.get("PC4", 0)])

    results = []
    for _, row in job_fit_df.iterrows():
        job_pc = np.array([row.get("PC1", 0), row.get("PC2", 0), row.get("PC3", 0), row.get("PC4", 0)])

        # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚’è¨ˆç®—
        distance = np.sqrt(np.sum((user_pc - job_pc) ** 2))

        # è·é›¢ã‚’0-100ã®ãƒãƒƒãƒåº¦ã‚¹ã‚³ã‚¢ã«å¤‰æ›ï¼ˆè·é›¢ãŒå°ã•ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
        # æœ€å¤§è·é›¢ã‚’ç´„50ã¨ä»®å®š
        match_score = max(0, min(100, 100 - (distance * 2)))

        results.append({
            "job_name": row.get("job_name", "ä¸æ˜"),
            "match_score": round(match_score, 1),
            "description": row.get("description", ""),
            "distance": round(distance, 2)
        })

    # ãƒãƒƒãƒåº¦ã§ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x["match_score"], reverse=True)

    return results[:top_n]


def reload_job_fit_data():
    """é©è·ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªãƒ­ãƒ¼ãƒ‰"""
    global job_fit_df
    if JOB_FIT_FILE.exists():
        job_fit_df = pd.read_excel(JOB_FIT_FILE)
    return job_fit_df


# =========================
# çµæœä¿å­˜ãƒ»èª­ã¿è¾¼ã¿
# =========================
def save_result(result_data: dict) -> str:
    """çµæœã‚’Firestoreã«ä¿å­˜ã—ã€IDã‚’è¿”ã™"""
    result_id = str(uuid.uuid4())[:8]
    result_data["id"] = result_id
    result_data["created_at"] = datetime.now().isoformat()

    # Firestoreç”¨ã«ãƒ‡ãƒ¼ã‚¿æ•´å½¢
    doc_data = {
        "id": result_id,
        "answers_raw": result_data.get("answers", []),
        "factors_55": result_data.get("features_55", {}),
        "factors_55_z": {},
        "flags": {
            "is_valid": True,
            "is_complete": True,
            "data_version": "3.0",
            "is_test": False
        },
        "meta": result_data.get("meta", {}),
        "pc_values": {
            "PC1": result_data.get("result", {}).get("PC1", 0),
            "PC2": result_data.get("result", {}).get("PC2", 0),
            "PC3": result_data.get("result", {}).get("PC3", 0),
            "PC4": result_data.get("result", {}).get("PC4", 0),
        },
        "report": result_data.get("report", {}),
        "result": result_data.get("result", {}),
        "consistency": result_data.get("consistency", {}),
        "badges": result_data.get("badges", []),
        "created_at": firestore.SERVER_TIMESTAMP,
        "updated_at": firestore.SERVER_TIMESTAMP,
    }

    # Firestoreã«ä¿å­˜
    db.collection(FIRESTORE_COLLECTION).document(result_id).set(doc_data)

    return result_id


def load_result(result_id: str) -> dict:
    """Firestoreã‹ã‚‰çµæœã‚’èª­ã¿è¾¼ã‚€"""
    doc = db.collection(FIRESTORE_COLLECTION).document(result_id).get()
    if not doc.exists:
        return None
    return doc.to_dict()


def list_results(company: str = None, department: str = None, status: str = None,
                  clinic: str = None, position: str = None, area: str = None) -> list:
    """Firestoreã‹ã‚‰çµæœä¸€è¦§ã‚’å–å¾—ï¼ˆæ‹¡å¼µãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œï¼‰"""
    results = []

    # Firestoreã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    query = db.collection(FIRESTORE_COLLECTION)

    # åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆFirestoreã§å¯èƒ½ãªã‚‚ã®ï¼‰
    # æ³¨æ„: Firestoreã¯è¤‡åˆã‚¯ã‚¨ãƒªã«åˆ¶é™ãŒã‚ã‚‹ãŸã‚ã€ä¸€éƒ¨ã¯Pythonå´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

    docs = query.stream()

    for doc in docs:
        data = doc.to_dict()
        meta = data.get("meta", {})
        tags = meta.get("tags", {})

        # Pythonå´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if company and meta.get("company") != company:
            continue
        if department and meta.get("department") != department:
            continue
        if status and meta.get("status") != status:
            continue
        if clinic and clinic not in tags.get("clinics", []):
            continue
        if position and position not in tags.get("positions", []):
            continue
        if area and area not in tags.get("areas", []):
            continue

        result_obj = data.get("result", {})

        # created_atã®å¤‰æ›ï¼ˆFirestore Timestampã®å ´åˆï¼‰
        created_at = data.get("created_at", "")
        if hasattr(created_at, 'isoformat'):
            created_at = created_at.isoformat()
        elif hasattr(created_at, 'timestamp'):
            created_at = datetime.fromtimestamp(created_at.timestamp()).isoformat()

        results.append({
            "id": data.get("id"),
            "name": meta.get("name", "ä¸æ˜"),
            "email": meta.get("email", ""),
            "company": meta.get("company", ""),
            "department": meta.get("department", ""),
            "tags": tags,
            "status": meta.get("status", "active"),
            "type": result_obj.get("type", ""),
            "result": result_obj,  # PC1ã€œPC4ã‚’å«ã‚€resultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            "meta": meta,
            "stress_tolerance": data.get("report", {}).get("stress_tolerance", 0),
            "consistency_score": data.get("consistency", {}).get("score", None),
            "created_at": created_at,
        })

    # æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x.get("created_at", ""), reverse=True)
    return results


def update_result_meta(result_id: str, updates: dict) -> bool:
    """Firestoreã®metaæƒ…å ±ã‚’æ›´æ–°"""
    doc_ref = db.collection(FIRESTORE_COLLECTION).document(result_id)
    doc = doc_ref.get()

    if not doc.exists:
        return False

    # metaãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ›´æ–°
    update_data = {"updated_at": firestore.SERVER_TIMESTAMP}
    for key, value in updates.items():
        if key in ["name", "email", "company", "department", "tags", "status"]:
            update_data[f"meta.{key}"] = value

    doc_ref.update(update_data)
    return True


# =========================
# CORS å¯¾å¿œ
# =========================
@app.after_request
def add_cors_headers(response):
    response.headers["Access-Control-Allow-Origin"] = "*"
    response.headers["Access-Control-Allow-Headers"] = "Content-Type"
    response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS"
    return response


# =========================
# API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# =========================

@app.get("/health")
def health():
    return "ok", 200


@app.get("/")
def index():
    return send_from_directory(".", "sakura_diagnosis.html")


@app.get("/admin")
def admin():
    return send_from_directory(".", "sakura_admin.html")


@app.get("/reference-admin")
def reference_admin():
    return send_from_directory(".", "sakura_reference_data_admin.html")


@app.get("/job-fit-admin")
def job_fit_admin():
    return send_from_directory(".", "sakura_job_fit_admin.html")


@app.route("/api/sakura_psych", methods=["POST", "OPTIONS"])
def api_sakura_psych():
    """è¨ºæ–­API"""
    if request.method == "OPTIONS":
        return ("", 204)

    data = request.get_json(silent=True) or {}
    meta = data.get("meta", {})

    raw_features_4axis = None
    features_55 = None

    answers = data.get("answers")
    if isinstance(answers, (list, tuple)):
        try:
            answers = [float(x) for x in answers]
        except Exception:
            return jsonify({
                "ok": False,
                "error": "answers ã¯æ•°å€¤ãƒªã‚¹ãƒˆã§æ¸¡ã—ã¦ãã ã•ã„ã€‚"
            }), 400

        try:
            raw_features_4axis = build_features_from_answers(answers)
        except Exception as e:
            app.logger.warning("4è»¸æŒ‡æ¨™ã®è¨ˆç®—ã«å¤±æ•—: %s", e)

        try:
            features_55 = build_55factor_features_from_answers(answers)
        except Exception as e:
            return jsonify({
                "ok": False,
                "error": f"answers ã‹ã‚‰55å› å­ã¸ã®å¤‰æ›ã§ã‚¨ãƒ©ãƒ¼: {str(e)}"
            }), 400

    if features_55 is None:
        maybe_features = data.get("features")
        if isinstance(maybe_features, dict):
            features_55 = maybe_features

    if features_55 is None:
        return jsonify({
            "ok": False,
            "error": "answersï¼ˆ72å›ç­”ï¼‰ã¾ãŸã¯ featuresï¼ˆ55å› å­dictï¼‰ã‚’æ¸¡ã—ã¦ãã ã•ã„ã€‚"
        }), 400

    try:
        result = compute_pc_and_type_from_features(features_55)
    except Exception as e:
        return jsonify({
            "ok": False,
            "error": f"ãƒ¢ãƒ‡ãƒ«è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}"
        }), 400

    report = generate_type_report(
        pc={
            "PC1": result["PC1"],
            "PC2": result["PC2"],
            "PC3": result["PC3"],
            "PC4": result["PC4"],
        },
        cluster_id=result["cluster_id"],
        type_label=result["type"],
        features_55=features_55,
    )

    # å›ç­”ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯
    consistency = calculate_consistency_score(answers) if answers else {"score": None}

    # metaã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¨­å®š
    if "status" not in meta:
        meta["status"] = "active"

    # é©è·ãƒ™ã‚¹ãƒˆ3ã‚’è¨ˆç®—
    job_fit = calculate_job_fit({
        "PC1": result["PC1"],
        "PC2": result["PC2"],
        "PC3": result["PC3"],
        "PC4": result["PC4"],
    }, top_n=3)

    # éš ã‚Œå±æ€§ãƒãƒƒã‚¸ã‚’ç”Ÿæˆ
    badges = generate_badges(features_55, {
        "PC1": result["PC1"],
        "PC2": result["PC2"],
        "PC3": result["PC3"],
        "PC4": result["PC4"],
    })

    # çµæœã‚’ä¿å­˜
    save_data = {
        "meta": meta,
        "result": result,
        "report": report,
        "features_55": features_55,
        "features_4axis": raw_features_4axis,
        "answers": answers,
        "consistency": consistency,
        "job_fit": job_fit,
        "badges": badges,
    }
    result_id = save_result(save_data)

    return jsonify({
        "ok": True,
        "result_id": result_id,
        "result": result,
        "report": report,
        "features_55": features_55,
        "features_4axis": raw_features_4axis,
        "meta": meta,
        "consistency": consistency,
        "job_fit": job_fit,
        "badges": badges,
    })


@app.route("/api/results", methods=["GET"])
def api_list_results():
    """ä¿å­˜ã•ã‚ŒãŸçµæœä¸€è¦§ã‚’å–å¾—ï¼ˆæ‹¡å¼µãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œï¼‰"""
    company = request.args.get("company")
    department = request.args.get("department")
    status = request.args.get("status")
    clinic = request.args.get("clinic")
    position = request.args.get("position")
    area = request.args.get("area")
    results = list_results(
        company=company,
        department=department,
        status=status,
        clinic=clinic,
        position=position,
        area=area
    )
    return jsonify({"ok": True, "results": results})


@app.route("/api/results/export", methods=["GET"])
def api_export_results():
    """çµæœã‚’CSVå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    company = request.args.get("company")
    status = request.args.get("status")
    results = list_results(company=company, status=status)

    output = io.StringIO()
    writer = csv.writer(output)

    # ãƒ˜ãƒƒãƒ€ãƒ¼
    writer.writerow([
        "ID", "åå‰", "ãƒ¡ãƒ¼ãƒ«", "æ‰€å±ã‚«ãƒ†ã‚´ãƒª", "éƒ¨ç½²", "ã‚¯ãƒªãƒ‹ãƒƒã‚¯", "è·ç¨®", "ã‚¨ãƒªã‚¢",
        "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", "ã‚¿ã‚¤ãƒ—", "ã‚¹ãƒˆãƒ¬ã‚¹è€æ€§", "ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢", "è¨ºæ–­æ—¥æ™‚"
    ])

    # ãƒ‡ãƒ¼ã‚¿
    for r in results:
        tags = r.get("tags", {})
        writer.writerow([
            r.get("id", ""),
            r.get("name", ""),
            r.get("email", ""),
            r.get("company", ""),
            r.get("department", ""),
            ", ".join(tags.get("clinics", [])),
            ", ".join(tags.get("positions", [])),
            ", ".join(tags.get("areas", [])),
            STATUS_LABELS.get(r.get("status", "active"), r.get("status", "")),
            r.get("type", ""),
            r.get("stress_tolerance", ""),
            r.get("consistency_score", ""),
            r.get("created_at", ""),
        ])

    output.seek(0)
    return Response(
        output.getvalue(),
        mimetype="text/csv",
        headers={"Content-Disposition": "attachment; filename=sakura_results.csv"}
    )


@app.route("/api/results/<result_id>", methods=["GET"])
def api_get_result(result_id):
    """ç‰¹å®šã®çµæœã‚’å–å¾—"""
    result = load_result(result_id)
    if result is None:
        return jsonify({"ok": False, "error": "çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}), 404
    return jsonify({"ok": True, "data": result})


@app.route("/api/results/<result_id>/meta", methods=["PUT", "OPTIONS"])
def api_update_result_meta(result_id):
    """çµæœã®metaæƒ…å ±ã‚’æ›´æ–°"""
    if request.method == "OPTIONS":
        return ("", 204)

    data = request.get_json(silent=True) or {}
    success = update_result_meta(result_id, data)

    if not success:
        return jsonify({"ok": False, "error": "çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}), 404

    return jsonify({"ok": True, "message": "æ›´æ–°ã—ã¾ã—ãŸ"})


@app.route("/api/results/<result_id>/status", methods=["PUT", "OPTIONS"])
def api_update_result_status(result_id):
    """çµæœã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°"""
    if request.method == "OPTIONS":
        return ("", 204)

    data = request.get_json(silent=True) or {}
    new_status = data.get("status")

    if new_status not in ["active", "leave", "retired"]:
        return jsonify({"ok": False, "error": "ç„¡åŠ¹ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§ã™"}), 400

    success = update_result_meta(result_id, {"status": new_status})

    if not success:
        return jsonify({"ok": False, "error": "çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}), 404

    return jsonify({"ok": True, "message": "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°ã—ã¾ã—ãŸ"})


@app.route("/api/company_categories", methods=["GET"])
def api_company_categories():
    """ä¼šç¤¾ã‚«ãƒ†ã‚´ãƒªä¸€è¦§ã‚’å–å¾—"""
    return jsonify({"ok": True, "categories": COMPANY_CATEGORIES})


@app.route("/api/tag_master", methods=["GET"])
def api_get_tag_master():
    """ã‚¿ã‚°ãƒã‚¹ã‚¿ãƒ¼ä¸€è¦§ã‚’å–å¾—"""
    global TAG_MASTER
    TAG_MASTER = load_tag_master()  # æœ€æ–°ã‚’èª­ã¿è¾¼ã¿
    return jsonify({"ok": True, "tags": TAG_MASTER, "status_labels": STATUS_LABELS})


@app.route("/api/tag_master", methods=["PUT", "OPTIONS"])
def api_update_tag_master():
    """ã‚¿ã‚°ãƒã‚¹ã‚¿ãƒ¼ã‚’æ›´æ–°"""
    if request.method == "OPTIONS":
        return ("", 204)

    global TAG_MASTER
    data = request.get_json(silent=True) or {}

    if "tags" in data:
        TAG_MASTER = data["tags"]
        save_tag_master(TAG_MASTER)

    return jsonify({"ok": True, "message": "ã‚¿ã‚°ãƒã‚¹ã‚¿ãƒ¼ã‚’æ›´æ–°ã—ã¾ã—ãŸ"})


# =========================
# å‚ç…§ãƒ‡ãƒ¼ã‚¿ç®¡ç†API
# =========================
@app.route("/api/reference-data", methods=["GET"])
def api_get_reference_data():
    """mean_std_55ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    data = mean_std_df.to_dict('records')
    return jsonify({"ok": True, "data": data})


@app.route("/api/reference-data/<feature>", methods=["PUT", "OPTIONS"])
def api_update_reference_data(feature):
    """ç‰¹å®šã®ç‰¹å¾´é‡ã®mean/stdã‚’æ›´æ–°"""
    if request.method == "OPTIONS":
        return ("", 204)

    global mean_std_df

    data = request.get_json(silent=True) or {}
    new_mean = data.get("mean")
    new_std = data.get("std")

    if new_mean is None or new_std is None:
        return jsonify({"ok": False, "error": "mean ã¨ std ãŒå¿…è¦ã§ã™"}), 400

    # DataFrameã‚’æ›´æ–°
    mask = mean_std_df["feature"] == feature
    if not mask.any():
        return jsonify({"ok": False, "error": f"ç‰¹å¾´é‡ '{feature}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}), 404

    mean_std_df.loc[mask, "mean"] = float(new_mean)
    mean_std_df.loc[mask, "std"] = float(new_std)

    # Excelã«ä¿å­˜
    mean_std_df.to_excel(DATA_DIR / "mean_std_55.xlsx", index=False)

    return jsonify({"ok": True, "message": f"'{feature}' ã‚’æ›´æ–°ã—ã¾ã—ãŸ"})


@app.route("/api/reference-data", methods=["POST", "OPTIONS"])
def api_add_reference_data():
    """æ–°ã—ã„ç‰¹å¾´é‡ã‚’è¿½åŠ """
    if request.method == "OPTIONS":
        return ("", 204)

    global mean_std_df

    data = request.get_json(silent=True) or {}
    feature = data.get("feature")
    new_mean = data.get("mean")
    new_std = data.get("std")

    if not feature or new_mean is None or new_std is None:
        return jsonify({"ok": False, "error": "feature, mean, std ãŒå¿…è¦ã§ã™"}), 400

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    if feature in mean_std_df["feature"].values:
        return jsonify({"ok": False, "error": f"ç‰¹å¾´é‡ '{feature}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™"}), 400

    # è¿½åŠ 
    new_row = pd.DataFrame([{"feature": feature, "mean": float(new_mean), "std": float(new_std)}])
    mean_std_df = pd.concat([mean_std_df, new_row], ignore_index=True)

    # Excelã«ä¿å­˜
    mean_std_df.to_excel(DATA_DIR / "mean_std_55.xlsx", index=False)

    return jsonify({"ok": True, "message": f"'{feature}' ã‚’è¿½åŠ ã—ã¾ã—ãŸ"})


@app.route("/api/reference-data/<feature>", methods=["DELETE", "OPTIONS"])
def api_delete_reference_data(feature):
    """ç‰¹å¾´é‡ã‚’å‰Šé™¤"""
    if request.method == "OPTIONS":
        return ("", 204)

    global mean_std_df

    mask = mean_std_df["feature"] == feature
    if not mask.any():
        return jsonify({"ok": False, "error": f"ç‰¹å¾´é‡ '{feature}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}), 404

    mean_std_df = mean_std_df[~mask].reset_index(drop=True)

    # Excelã«ä¿å­˜
    mean_std_df.to_excel(DATA_DIR / "mean_std_55.xlsx", index=False)

    return jsonify({"ok": True, "message": f"'{feature}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ"})


# =========================
# é©è·ãƒ‡ãƒ¼ã‚¿ç®¡ç†API
# =========================
@app.route("/api/job-fit-profiles", methods=["GET"])
def api_list_job_fit_profiles():
    """é©è·ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    global job_fit_df
    job_fit_df = reload_job_fit_data()

    if job_fit_df is None or job_fit_df.empty:
        return jsonify({"ok": True, "profiles": []})

    profiles = job_fit_df.to_dict(orient="records")
    return jsonify({"ok": True, "profiles": profiles})


@app.route("/api/job-fit-profiles/<job_name>", methods=["PUT", "OPTIONS"])
def api_update_job_fit_profile(job_name):
    """é©è·ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
    if request.method == "OPTIONS":
        return ("", 204)

    global job_fit_df

    data = request.get_json(silent=True) or {}

    mask = job_fit_df["job_name"] == job_name
    if not mask.any():
        return jsonify({"ok": False, "error": f"è·ç¨® '{job_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}), 404

    # æ›´æ–°
    for col in ["PC1", "PC2", "PC3", "PC4", "description"]:
        if col in data:
            job_fit_df.loc[mask, col] = data[col]

    # Excelã«ä¿å­˜
    job_fit_df.to_excel(JOB_FIT_FILE, index=False)

    return jsonify({"ok": True, "message": f"'{job_name}' ã‚’æ›´æ–°ã—ã¾ã—ãŸ"})


@app.route("/api/job-fit-profiles", methods=["POST", "OPTIONS"])
def api_add_job_fit_profile():
    """æ–°ã—ã„é©è·ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ """
    if request.method == "OPTIONS":
        return ("", 204)

    global job_fit_df

    data = request.get_json(silent=True) or {}
    job_name = data.get("job_name")

    if not job_name:
        return jsonify({"ok": False, "error": "job_name ãŒå¿…è¦ã§ã™"}), 400

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    if job_name in job_fit_df["job_name"].values:
        return jsonify({"ok": False, "error": f"è·ç¨® '{job_name}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™"}), 400

    # è¿½åŠ 
    new_row = pd.DataFrame([{
        "job_name": job_name,
        "PC1": float(data.get("PC1", 0)),
        "PC2": float(data.get("PC2", 0)),
        "PC3": float(data.get("PC3", 0)),
        "PC4": float(data.get("PC4", 0)),
        "description": data.get("description", ""),
    }])
    job_fit_df = pd.concat([job_fit_df, new_row], ignore_index=True)

    # Excelã«ä¿å­˜
    job_fit_df.to_excel(JOB_FIT_FILE, index=False)

    return jsonify({"ok": True, "message": f"'{job_name}' ã‚’è¿½åŠ ã—ã¾ã—ãŸ"})


@app.route("/api/job-fit-profiles/<job_name>", methods=["DELETE", "OPTIONS"])
def api_delete_job_fit_profile(job_name):
    """é©è·ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    if request.method == "OPTIONS":
        return ("", 204)

    global job_fit_df

    mask = job_fit_df["job_name"] == job_name
    if not mask.any():
        return jsonify({"ok": False, "error": f"è·ç¨® '{job_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}), 404

    job_fit_df = job_fit_df[~mask].reset_index(drop=True)

    # Excelã«ä¿å­˜
    job_fit_df.to_excel(JOB_FIT_FILE, index=False)

    return jsonify({"ok": True, "message": f"'{job_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ"})


# =========================
# ã‚¿ã‚°åˆ¥çµ±è¨ˆAPI
# =========================
@app.route("/api/tag-statistics", methods=["GET"])
def api_tag_statistics():
    """ã‚¿ã‚°åˆ¥ã®å¹³å‡PCã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¦è¿”ã™"""
    # å…¨çµæœã‚’èª­ã¿è¾¼ã¿
    all_results = []
    for filepath in RESULTS_DIR.glob("*.json"):
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
            result = data.get("result", {})
            meta = data.get("meta", {})
            tags = meta.get("tags", {})

            # PCã‚¹ã‚³ã‚¢ãŒã‚ã‚‹çµæœã®ã¿
            if result.get("PC1") is not None:
                all_results.append({
                    "PC1": result.get("PC1", 0),
                    "PC2": result.get("PC2", 0),
                    "PC3": result.get("PC3", 0),
                    "PC4": result.get("PC4", 0),
                    "company": meta.get("company", ""),
                    "tags": tags,
                })

    if not all_results:
        return jsonify({"ok": True, "statistics": {}, "total_count": 0})

    # ã‚¿ã‚°ãƒã‚¹ã‚¿ãƒ¼ã‚’å–å¾—
    tag_master = load_tag_master()

    statistics = {}

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«é›†è¨ˆ
    for category, values in tag_master.items():
        if category == "status":
            continue  # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¯é™¤å¤–

        statistics[category] = {}

        for tag_value in values:
            # ã“ã®ã‚¿ã‚°ã‚’æŒã¤çµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿
            matching = []
            for r in all_results:
                tag_list = r["tags"].get(category, [])
                if tag_value in tag_list:
                    matching.append(r)

            if matching:
                avg_pc1 = sum(r["PC1"] for r in matching) / len(matching)
                avg_pc2 = sum(r["PC2"] for r in matching) / len(matching)
                avg_pc3 = sum(r["PC3"] for r in matching) / len(matching)
                avg_pc4 = sum(r["PC4"] for r in matching) / len(matching)

                statistics[category][tag_value] = {
                    "count": len(matching),
                    "PC1": round(avg_pc1, 2),
                    "PC2": round(avg_pc2, 2),
                    "PC3": round(avg_pc3, 2),
                    "PC4": round(avg_pc4, 2),
                }

    # æ‰€å±ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚‚é›†è¨ˆ
    statistics["company"] = {}
    for category in COMPANY_CATEGORIES:
        matching = [r for r in all_results if r["company"] == category]
        if matching:
            avg_pc1 = sum(r["PC1"] for r in matching) / len(matching)
            avg_pc2 = sum(r["PC2"] for r in matching) / len(matching)
            avg_pc3 = sum(r["PC3"] for r in matching) / len(matching)
            avg_pc4 = sum(r["PC4"] for r in matching) / len(matching)

            statistics["company"][category] = {
                "count": len(matching),
                "PC1": round(avg_pc1, 2),
                "PC2": round(avg_pc2, 2),
                "PC3": round(avg_pc3, 2),
                "PC4": round(avg_pc4, 2),
            }

    return jsonify({
        "ok": True,
        "statistics": statistics,
        "total_count": len(all_results)
    })


@app.route("/api/tag-statistics/import-to-profiles", methods=["POST", "OPTIONS"])
def api_import_tag_to_profiles():
    """ã‚¿ã‚°ã®çµ±è¨ˆå€¤ã‚’é©è·ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    if request.method == "OPTIONS":
        return ("", 204)

    global job_fit_df

    data = request.get_json(silent=True) or {}
    tag_name = data.get("tag_name")
    pc1 = data.get("PC1", 0)
    pc2 = data.get("PC2", 0)
    pc3 = data.get("PC3", 0)
    pc4 = data.get("PC4", 0)
    count = data.get("count", 0)

    if not tag_name:
        return jsonify({"ok": False, "error": "tag_name ãŒå¿…è¦ã§ã™"}), 400

    description = f"ã‚¿ã‚°ã€Œ{tag_name}ã€ã®å¹³å‡å€¤ï¼ˆ{count}åï¼‰"

    # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
    if tag_name in job_fit_df["job_name"].values:
        # æ›´æ–°
        mask = job_fit_df["job_name"] == tag_name
        job_fit_df.loc[mask, "PC1"] = pc1
        job_fit_df.loc[mask, "PC2"] = pc2
        job_fit_df.loc[mask, "PC3"] = pc3
        job_fit_df.loc[mask, "PC4"] = pc4
        job_fit_df.loc[mask, "description"] = description
        message = f"'{tag_name}' ã‚’æ›´æ–°ã—ã¾ã—ãŸ"
    else:
        # è¿½åŠ 
        new_row = pd.DataFrame([{
            "job_name": tag_name,
            "PC1": pc1,
            "PC2": pc2,
            "PC3": pc3,
            "PC4": pc4,
            "description": description,
        }])
        job_fit_df = pd.concat([job_fit_df, new_row], ignore_index=True)
        message = f"'{tag_name}' ã‚’è¿½åŠ ã—ã¾ã—ãŸ"

    # Excelã«ä¿å­˜
    job_fit_df.to_excel(JOB_FIT_FILE, index=False)

    return jsonify({"ok": True, "message": message})


# =========================
# ãƒ¡ãƒ¼ãƒ«é€ä¿¡
# =========================
def send_mail(to_addrs, subject, body):
    if not (SMTP_HOST and SMTP_USER and SMTP_PASS):
        app.logger.warning("SMTP settings are not configured. Skip sending email.")
        return

    if isinstance(to_addrs, str):
        to_addrs = [to_addrs]

    msg = EmailMessage()
    msg["Subject"] = subject
    msg["From"] = MAIL_FROM or SMTP_USER
    msg["To"] = ", ".join(to_addrs)
    msg.set_content(body)

    with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as smtp:
        smtp.starttls()
        smtp.login(SMTP_USER, SMTP_PASS)
        smtp.send_message(msg)


@app.post("/api/send-email")
def api_send_email():
    """è¨ºæ–­çµæœã‚’ãƒ¡ãƒ¼ãƒ«é€ä¿¡"""
    try:
        data = request.get_json(force=True)
    except Exception:
        return jsonify({"success": False, "error": "invalid json"}), 400

    result_id = data.get("result_id")
    email = data.get("email")

    if not email:
        return jsonify({"success": False, "error": "email is required"}), 400

    if not result_id:
        return jsonify({"success": False, "error": "result_id is required"}), 400

    result_data = load_result(result_id)
    if not result_data:
        return jsonify({"success": False, "error": "çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}), 404

    meta = result_data.get("meta", {})
    name = meta.get("name", "å—æ¤œè€…")
    report = result_data.get("report", {})
    result = result_data.get("result", {})

    type_label = result.get("type", "")
    type_name = f"ã‚¿ã‚¤ãƒ—{type_label}"
    
    # è»¸æƒ…å ±
    axis_info = report.get("axis_info", [])
    axis_text = "\n".join([
        f"ãƒ»{a['name']}ï¼ˆ{a['description'][:10]}...ï¼‰: {a['level']}"
        for a in axis_info
    ])

    user_subject = f"[ã•ãã‚‰ä¼š] å¿ƒç†è¨ºæ–­çµæœã®ã”æ¡ˆå†…"
    user_body = f"""{name} æ§˜

ã“ã®åº¦ã¯å¿ƒç†è¨ºæ–­ã«ã”å”åŠ›ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚
ã‚ãªãŸã®è¨ºæ–­çµæœã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

â–  è¨ºæ–­ã‚¿ã‚¤ãƒ—
{report.get('summary', '')}

â–  ä¸€è¨€ã§ã¾ã¨ã‚ã‚‹ã¨
{report.get('one_liner', '')}

â–  å„è»¸ã®å‚¾å‘
{axis_text}

â–  å¼·ã¿
{"ã€".join(report.get('strengths', []))}

â–  æ³¨æ„ç‚¹
{"ã€".join(report.get('weaknesses', []))}

â–  ã‚¹ãƒˆãƒ¬ã‚¹è€æ€§
{report.get('stress_tolerance', 5)}ç‚¹ï¼ˆ10ç‚¹æº€ç‚¹ï¼‰

â€» æœ¬è¨ºæ–­ã¯ã€ã•ãã‚‰ä¼šå†…ã§ã®é…ç½®ãƒ»è‚²æˆãƒ»1on1é¢è«‡ã«æ´»ç”¨ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã‚ã‚Šã€
   åˆå¦ã‚„è©•ä¾¡ã‚’ç›´æ¥æ±ºã‚ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

åŒ»ç™‚æ³•äººã•ãã‚‰ä¼š
"""

    try:
        send_mail(email, user_subject, user_body)
        if MAIL_HR:
            hr_subject = f"[ã•ãã‚‰ä¼š] {name} ã•ã‚“ã®å¿ƒç†è¨ºæ–­çµæœ"
            send_mail(MAIL_HR, hr_subject, user_body)
    except Exception as e:
        app.logger.exception("send_mail failed: %s", e)

    return jsonify({"success": True})


# =========================
# ç›¸æ€§è¨ºæ–­API
# =========================

# ã‚¿ã‚¤ãƒ—ç›¸æ€§ãƒãƒˆãƒªã‚¯ã‚¹ï¼ˆåŸºæœ¬ã‚¹ã‚³ã‚¢ï¼‰
# D: å°å‹, S: å’Œå‹, C: ç†å‹, P: é™½å‹
TYPE_COMPATIBILITY_MATRIX = {
    ("D", "D"): 70,  # åŒã‚¿ã‚¤ãƒ—: ä¼¼ã™ãã¦è¡çªã—ã‚„ã™ã„
    ("D", "S"): 85,  # å°Ã—å’Œ: ãƒªãƒ¼ãƒ€ãƒ¼ã¨ã‚µãƒãƒ¼ã‚¿ãƒ¼ã§å¥½ç›¸æ€§
    ("D", "C"): 75,  # å°Ã—ç†: è«–ç†çš„ãªè£œä½å½¹ã¨ã—ã¦æ©Ÿèƒ½
    ("D", "P"): 60,  # å°Ã—é™½: ä¸¡æ–¹ä¸»å¼µãŒå¼·ãè¡çªã—ã‚„ã™ã„
    ("S", "D"): 85,
    ("S", "S"): 75,  # åŒã‚¿ã‚¤ãƒ—: ç©ã‚„ã‹ã ãŒæ±ºæ–­åŠ›ã«æ¬ ã‘ã‚‹
    ("S", "C"): 90,  # å’ŒÃ—ç†: æœ€ã‚‚å®‰å®šã—ãŸçµ„ã¿åˆã‚ã›
    ("S", "P"): 70,  # å’ŒÃ—é™½: é™½ãŒãƒªãƒ¼ãƒ‰ã—ã™ãã‚‹å‚¾å‘
    ("C", "D"): 75,
    ("C", "S"): 90,
    ("C", "C"): 70,  # åŒã‚¿ã‚¤ãƒ—: æ…é‡ã™ãã¦é€²ã¾ãªã„
    ("C", "P"): 80,  # ç†Ã—é™½: åˆ†æã¨ç™ºä¿¡ã®ãƒãƒ©ãƒ³ã‚¹
    ("P", "D"): 60,
    ("P", "S"): 70,
    ("P", "C"): 80,
    ("P", "P"): 65,  # åŒã‚¿ã‚¤ãƒ—: ç«¶äº‰çš„ã«ãªã‚Šã‚„ã™ã„
}

# ã‚¿ã‚¤ãƒ—ç›¸æ€§ã‚³ãƒ¡ãƒ³ãƒˆ
TYPE_COMPATIBILITY_COMMENTS = {
    ("D", "D"): {
        "summary": "ãƒªãƒ¼ãƒ€ãƒ¼åŒå£«ã®çµ„ã¿åˆã‚ã›",
        "strengths": ["æ±ºæ–­åŠ›ãŒé«˜ãã€ç‰©äº‹ã‚’æ¨é€²ã§ãã‚‹", "ç›®æ¨™é”æˆã¸ã®æ„è­˜ãŒå¼·ã„"],
        "challenges": ["ä¸»å°æ¨©äº‰ã„ã«ãªã‚Šã‚„ã™ã„", "æ„è¦‹ãŒå¯¾ç«‹ã—ãŸéš›ã«è­²ã‚Šåˆã„ãŒé›£ã—ã„"],
        "advice": "å½¹å‰²åˆ†æ‹…ã‚’æ˜ç¢ºã«ã—ã€ãã‚Œãã‚Œã®æ‹…å½“é ˜åŸŸã‚’å°Šé‡ã™ã‚‹ã“ã¨ãŒå¤§åˆ‡ã§ã™"
    },
    ("D", "S"): {
        "summary": "ãƒªãƒ¼ãƒ€ãƒ¼ã¨ã‚µãƒãƒ¼ã‚¿ãƒ¼ã®ç†æƒ³çš„ãªçµ„ã¿åˆã‚ã›",
        "strengths": ["å°å‹ãŒãƒªãƒ¼ãƒ‰ã—ã€å’Œå‹ãŒãƒ•ã‚©ãƒ­ãƒ¼", "ãƒãƒ¼ãƒ ã¨ã—ã¦ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã‚‹"],
        "challenges": ["å’Œå‹ã®æ„è¦‹ãŒåŸ‹ã‚‚ã‚Œã‚„ã™ã„"],
        "advice": "å°å‹ã¯å’Œå‹ã®æ„è¦‹ã‚’ç©æ¥µçš„ã«èãå§¿å‹¢ã‚’æŒã¡ã¾ã—ã‚‡ã†"
    },
    ("D", "C"): {
        "summary": "æ±ºæ–­åŠ›ã¨åˆ†æåŠ›ã®çµ„ã¿åˆã‚ã›",
        "strengths": ["å°å‹ã®æ±ºæ–­ã‚’ç†å‹ãŒè«–ç†çš„ã«è£œä½", "è¨ˆç”»ã¨å®Ÿè¡Œã®ãƒãƒ©ãƒ³ã‚¹"],
        "challenges": ["ç†å‹ã®æ…é‡ã•ãŒå°å‹ã«ã¯é…ãæ„Ÿã˜ã‚‹ã“ã¨ã‚‚"],
        "advice": "ç†å‹ã®åˆ†æã‚’å°Šé‡ã—ã¤ã¤ã€ã‚¹ãƒ”ãƒ¼ãƒ‰æ„Ÿã‚‚æ„è­˜ã—ã¾ã—ã‚‡ã†"
    },
    ("D", "P"): {
        "summary": "äºŒäººã¨ã‚‚å‰ã«å‡ºãŸã„ã‚¿ã‚¤ãƒ—",
        "strengths": ["ã‚¨ãƒãƒ«ã‚®ãƒƒã‚·ãƒ¥ã§æ¨é€²åŠ›ãŒã‚ã‚‹", "ç©æ¥µçš„ã«ç‰©äº‹ã‚’é€²ã‚ã‚‰ã‚Œã‚‹"],
        "challenges": ["ä¸»å¼µãŒã¶ã¤ã‹ã‚Šã‚„ã™ã„", "è­²ã‚Šåˆã„ãŒé›£ã—ã„å ´é¢ã‚‚"],
        "advice": "ãŠäº’ã„ã®å¼·ã¿ã‚’èªã‚åˆã„ã€ç«¶äº‰ã‚ˆã‚Šå”åŠ›ã‚’æ„è­˜ã—ã¾ã—ã‚‡ã†"
    },
    ("S", "S"): {
        "summary": "ç©ã‚„ã‹ã§å”èª¿çš„ãªçµ„ã¿åˆã‚ã›",
        "strengths": ["äº‰ã„ãŒå°‘ãªãç©ã‚„ã‹ãªé–¢ä¿‚", "äº’ã„ã‚’å°Šé‡ã—åˆãˆã‚‹"],
        "challenges": ["æ±ºæ–­ãŒé…ããªã‚ŠãŒã¡", "ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ãŒä¸è¶³ã™ã‚‹ã“ã¨ã‚‚"],
        "advice": "ã©ã¡ã‚‰ã‹ãŒæ„è­˜çš„ã«æ±ºæ–­å½¹ã‚’æ‹…ã†ã¨è‰¯ã„ã§ã—ã‚‡ã†"
    },
    ("S", "C"): {
        "summary": "æœ€ã‚‚å®‰å®šæ„Ÿã®ã‚ã‚‹çµ„ã¿åˆã‚ã›",
        "strengths": ["å’Œå‹ã®å”èª¿æ€§ã¨ç†å‹ã®æ­£ç¢ºã•ãŒå™›ã¿åˆã†", "å®‰å®šã—ãŸãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯"],
        "challenges": ["å¤‰åŒ–ã¸ã®å¯¾å¿œãŒé…ããªã‚‹å¯èƒ½æ€§"],
        "advice": "ãŠäº’ã„ã®å¼·ã¿ã‚’æ´»ã‹ã—ã€å®‰å®šæ„Ÿã‚’æ­¦å™¨ã«ã—ã¾ã—ã‚‡ã†"
    },
    ("S", "P"): {
        "summary": "ã‚µãƒãƒ¼ã‚¿ãƒ¼ã¨ç™ºä¿¡è€…ã®çµ„ã¿åˆã‚ã›",
        "strengths": ["é™½å‹ãŒå¼•ã£å¼µã‚Šã€å’Œå‹ãŒãƒ•ã‚©ãƒ­ãƒ¼", "å¤–å‘çš„ãªãƒãƒ©ãƒ³ã‚¹"],
        "challenges": ["é™½å‹ã®ãƒšãƒ¼ã‚¹ã«å’Œå‹ãŒåˆã‚ã›ã™ãã‚‹ã“ã¨ã‚‚"],
        "advice": "å’Œå‹ã‚‚è‡ªåˆ†ã®æ„è¦‹ã‚’ä¼ãˆã‚‹ã“ã¨ã‚’æ„è­˜ã—ã¾ã—ã‚‡ã†"
    },
    ("C", "C"): {
        "summary": "åˆ†ææ´¾åŒå£«ã®çµ„ã¿åˆã‚ã›",
        "strengths": ["è«–ç†çš„ã§æ­£ç¢ºãªåˆ¤æ–­ãŒã§ãã‚‹", "ãƒŸã‚¹ãŒå°‘ãªã„"],
        "challenges": ["æ…é‡ã«ãªã‚Šã™ãã¦é€²ã¾ãªã„", "æ±ºæ–­ã«æ™‚é–“ãŒã‹ã‹ã‚‹"],
        "advice": "ã‚ã‚‹ç¨‹åº¦ã§åˆ¤æ–­ã‚’ä¸‹ã™ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’æ±ºã‚ã¦ãŠãã¾ã—ã‚‡ã†"
    },
    ("C", "P"): {
        "summary": "åˆ†æåŠ›ã¨ç™ºä¿¡åŠ›ã®çµ„ã¿åˆã‚ã›",
        "strengths": ["ç†å‹ã®åˆ†æã‚’é™½å‹ãŒç™ºä¿¡", "å†…å®¹ã¨ä¼ãˆæ–¹ã®ãƒãƒ©ãƒ³ã‚¹"],
        "challenges": ["ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é•ã„ã§æ„è¦‹ãŒåˆ†ã‹ã‚Œã‚‹ã“ã¨ã‚‚"],
        "advice": "ç†å‹ã¯ç™ºä¿¡ã‚’ã€é™½å‹ã¯æ·±æ˜ã‚Šã‚’äº’ã„ã«å­¦ã³åˆã„ã¾ã—ã‚‡ã†"
    },
    ("P", "P"): {
        "summary": "ç¤¾äº¤çš„ãªã‚¨ãƒãƒ«ã‚®ãƒƒã‚·ãƒ¥ã‚³ãƒ³ãƒ“",
        "strengths": ["æ˜ã‚‹ãæ´»æ°—ã®ã‚ã‚‹é›°å›²æ°—", "ç™ºä¿¡åŠ›ãƒ»å½±éŸ¿åŠ›ãŒå¼·ã„"],
        "challenges": ["ç«¶äº‰çš„ã«ãªã‚Šã‚„ã™ã„", "åœ°é“ãªä½œæ¥­ãŒç–ã‹ã«ãªã‚‹ã“ã¨ã‚‚"],
        "advice": "å½¹å‰²ã‚’åˆ†ã‘ã¦ã€ãŠäº’ã„ã®æ´»èºã®å ´ã‚’ä½œã‚Šã¾ã—ã‚‡ã†"
    },
}


def calculate_compatibility(data1: dict, data2: dict) -> dict:
    """
    2äººã®è¨ºæ–­çµæœã‹ã‚‰ç›¸æ€§ã‚’è¨ˆç®—ã™ã‚‹

    Args:
        data1: 1äººç›®ã®è¨ºæ–­ãƒ‡ãƒ¼ã‚¿ï¼ˆresult, meta, reportã‚’å«ã‚€ï¼‰
        data2: 2äººç›®ã®è¨ºæ–­ãƒ‡ãƒ¼ã‚¿

    Returns:
        ç›¸æ€§ã‚¹ã‚³ã‚¢ã¨è©³ç´°æƒ…å ±
    """
    result1 = data1.get("result", {})
    result2 = data2.get("result", {})

    type1 = result1.get("type", "S")
    type2 = result2.get("type", "S")

    # ã‚¿ã‚¤ãƒ—ç›¸æ€§ã®åŸºæœ¬ã‚¹ã‚³ã‚¢
    type_score = TYPE_COMPATIBILITY_MATRIX.get((type1, type2), 70)

    # PCã‚¹ã‚³ã‚¢ã®è·é›¢è¨ˆç®—
    pc1 = np.array([
        result1.get("PC1", 0),
        result1.get("PC2", 0),
        result1.get("PC3", 0),
        result1.get("PC4", 0)
    ])
    pc2 = np.array([
        result2.get("PC1", 0),
        result2.get("PC2", 0),
        result2.get("PC3", 0),
        result2.get("PC4", 0)
    ])

    distance = float(np.sqrt(np.sum((pc1 - pc2) ** 2)))

    # è·é›¢ã«ã‚ˆã‚‹èª¿æ•´ï¼ˆè¿‘ã„ã»ã©ãƒ—ãƒ©ã‚¹ã€é ã„ã»ã©ãƒã‚¤ãƒŠã‚¹ï¼‰
    # è·é›¢0ã§+20ã€è·é›¢100ã§-20
    adjustment = max(-20, min(20, (50 - distance) / 2.5))

    # æœ€çµ‚ã‚¹ã‚³ã‚¢
    final_score = type_score + adjustment
    final_score = max(0, min(100, final_score))

    # ç›¸æ€§ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—
    comment_key = (type1, type2)
    if comment_key not in TYPE_COMPATIBILITY_COMMENTS:
        comment_key = (type2, type1)

    comments = TYPE_COMPATIBILITY_COMMENTS.get(comment_key, {
        "summary": "ç›¸æ€§æƒ…å ±",
        "strengths": [],
        "challenges": [],
        "advice": ""
    })

    # ã‚¹ã‚³ã‚¢ã«å¿œã˜ãŸãƒ©ãƒ™ãƒ«
    if final_score >= 85:
        score_label = "ã¨ã¦ã‚‚è‰¯ã„"
        score_emoji = "ğŸ’•"
    elif final_score >= 75:
        score_label = "è‰¯ã„"
        score_emoji = "ğŸ˜Š"
    elif final_score >= 65:
        score_label = "æ™®é€š"
        score_emoji = "ğŸ™‚"
    elif final_score >= 55:
        score_label = "ã‚„ã‚„æ³¨æ„"
        score_emoji = "ğŸ¤”"
    else:
        score_label = "è¦é…æ…®"
        score_emoji = "âš ï¸"

    return {
        "score": round(final_score, 1),
        "score_label": score_label,
        "score_emoji": score_emoji,
        "type_score": type_score,
        "distance": round(distance, 2),
        "adjustment": round(adjustment, 1),
        "type1": type1,
        "type2": type2,
        "comments": comments
    }


@app.route("/api/compatibility", methods=["POST", "OPTIONS"])
def api_compatibility():
    """2äººã®è¨ºæ–­çµæœã®ç›¸æ€§ã‚’è¨ˆç®—"""
    if request.method == "OPTIONS":
        return "", 204

    try:
        body = request.get_json()
        id1 = body.get("id1")
        id2 = body.get("id2")

        if not id1 or not id2:
            return jsonify({"ok": False, "error": "id1ã¨id2ãŒå¿…è¦ã§ã™"}), 400

        if id1 == id2:
            return jsonify({"ok": False, "error": "åŒã˜äººã¯æ¯”è¼ƒã§ãã¾ã›ã‚“"}), 400

        # Firestoreã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        doc1 = db.collection(FIRESTORE_COLLECTION).document(id1).get()
        doc2 = db.collection(FIRESTORE_COLLECTION).document(id2).get()

        if not doc1.exists:
            return jsonify({"ok": False, "error": f"ID {id1} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}), 404
        if not doc2.exists:
            return jsonify({"ok": False, "error": f"ID {id2} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}), 404

        data1 = doc1.to_dict()
        data2 = doc2.to_dict()

        # ç›¸æ€§è¨ˆç®—
        compatibility = calculate_compatibility(data1, data2)

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«äººç‰©æƒ…å ±ã‚‚å«ã‚ã‚‹
        meta1 = data1.get("meta", {})
        meta2 = data2.get("meta", {})
        result1 = data1.get("result", {})
        result2 = data2.get("result", {})
        report1 = data1.get("report", {})
        report2 = data2.get("report", {})

        return jsonify({
            "ok": True,
            "compatibility": compatibility,
            "person1": {
                "id": id1,
                "name": meta1.get("name", "ä¸æ˜"),
                "type": result1.get("type", "S"),
                "type_name": AXIS_NAMES_JP.get(f"PC{['D','S','C','P'].index(result1.get('type', 'S'))+1}", ""),
                "PC1": result1.get("PC1", 0),
                "PC2": result1.get("PC2", 0),
                "PC3": result1.get("PC3", 0),
                "PC4": result1.get("PC4", 0),
                "axis_info": report1.get("axis_info", []),
                "stress_tolerance": report1.get("stress_tolerance", 5)
            },
            "person2": {
                "id": id2,
                "name": meta2.get("name", "ä¸æ˜"),
                "type": result2.get("type", "S"),
                "type_name": AXIS_NAMES_JP.get(f"PC{['D','S','C','P'].index(result2.get('type', 'S'))+1}", ""),
                "PC1": result2.get("PC1", 0),
                "PC2": result2.get("PC2", 0),
                "PC3": result2.get("PC3", 0),
                "PC4": result2.get("PC4", 0),
                "axis_info": report2.get("axis_info", []),
                "stress_tolerance": report2.get("stress_tolerance", 5)
            }
        })

    except Exception as e:
        app.logger.exception("compatibility API error: %s", e)
        return jsonify({"ok": False, "error": str(e)}), 500


# =========================
# ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç”¨
# =========================
if __name__ == "__main__":
    port = int(os.environ.get("PORT", "8080"))
    app.run(host="0.0.0.0", port=port, debug=True)
